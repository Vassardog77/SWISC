{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This part of the package processes all the recording data and \n",
    "saves processed files as x and y numpy archives. Here full set of features\n",
    "is implemented, but, if necessary other feature engeneering steps can be added\"\"\"\n",
    "\n",
    "# import housekeeping files\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "# data processing packages\n",
    "import hdf5storage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neo\n",
    "# signal processing packages\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, filtfilt, decimate\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# future engeneering packages\n",
    "\n",
    "import pyfftw\n",
    "from scipy.stats import kurtosis       # kurtosis function\n",
    "from scipy.stats import skew           # skewness function\n",
    "\n",
    "# config file has all the processing parameters for the data files\n",
    "import config\n",
    "from IPython.display import display\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#unit tests added\n",
    "import unit_tests_code\n",
    "\n",
    "# path to stored files\n",
    "path='C:/Users/benmo/Desktop/TestMats/'\n",
    "decimated_folder_path=f'D:/deletethisfolder/'\n",
    "\"\"\"Upload all variables from config.py file\"\"\"\n",
    "\n",
    "recording_length_hours=config.recording_length_hours\n",
    "sampling_freq=config.sampling_freq\n",
    "epoch_length=config.epoch_length\n",
    "recording_length_seconds=config.recording_length_seconds\n",
    "target_epoch_count=config.target_epoch_count\n",
    "channels=config.channels\n",
    "sampling_freq_dec=config.sampling_freq_dec\n",
    "\n",
    "\n",
    "# recording_length_hours=12\n",
    "# sampling_freq=2000\n",
    "# epoch_length=4\n",
    "# target_epoch_count=int(epoch_length*recording_length_hours*3600)\n",
    "# channels=['ECog','EMG','HPCL','HPCR']\n",
    "\n",
    "epoch_samples_dec=sampling_freq_dec*epoch_length\n",
    "\n",
    "b,a=signal.butter(1,[1],'high', fs=sampling_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressbar(it, prefix=\"\", size=60, out=sys.stdout): # Python3.6+\n",
    "    # imbr\n",
    "    # https://stackoverflow.com/questions/3160699/python-progress-bar/26761413#26761413\n",
    "    count = len(it)\n",
    "    start = time.time() # time estimate start\n",
    "    def show(j):\n",
    "        x = int(size*j/count)\n",
    "        # time estimate calculation and string\n",
    "        remaining = ((time.time() - start) / j) * (count - j)        \n",
    "        mins, sec = divmod(remaining, 60) # limited to minutes\n",
    "        time_str = f\"{int(mins):02}:{sec:03.1f}\"\n",
    "        dh_progress.update(f\"{prefix}[{u'█'*x}{('.'*(size-x))}] {j}/{count} Est wait {time_str}\")\n",
    "    show(0.1) # avoid div/0 \n",
    "    for i, item in enumerate(it):\n",
    "        yield item\n",
    "        show(i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function goes through nested file folders and return all the files as a list\n",
    "# THe list is formatted as [complete_path, annotation type folder name, file_name]\n",
    "def return_file_list_from_server (server):\n",
    "    file_list=[]\n",
    "    folderlist_server = os.listdir(server)  # get cohort folders\n",
    "    \n",
    "    # get all subfolders for annotation types in '/NoNo/, /NoSz/, /SlNo/, /SlSl/ format from Spike2 export'\n",
    "    for folder in folderlist_server:\n",
    "        # get the type level folder\n",
    "        types=os.listdir(server+\"/\"+folder)\n",
    "        for annotation_type in types:\n",
    "            type_folder=server+\"/\"+folder+\"/\"+annotation_type\n",
    "            type_folder_file_list=os.listdir(type_folder)\n",
    "            for mat_file in type_folder_file_list:\n",
    "                # print  (mat_file)\n",
    "                file_list.append([server+\"/\"+folder+\"/\"+annotation_type+\"/\"+mat_file, annotation_type, mat_file])\n",
    "    # total count\n",
    "    return file_list        \n",
    "    \n",
    " # Optional Funciton that prints out names of the folders and files found:\n",
    "def describe_the_files(files_found):\n",
    "    print('  ')\n",
    "    print(\"Total files found {}\".format(len(files_found)))\n",
    "    print(\"   \")\n",
    "    recording_formats=set()\n",
    "    recording_names=[]\n",
    "    for recording in files_found:\n",
    "        if recording[1] not in recording_formats:\n",
    "            recording_formats.add(recording[1])\n",
    "        recording_names.append(recording[2])\n",
    "\n",
    "    print('Recording formats found {0}: {1}, '.format(len(recording_formats),recording_formats))\n",
    "    print(\"    \")\n",
    "    #print('Complete list of the files found: \\n{}'.format(\"\\n\".join(recording_names)))\n",
    "\n",
    "# This function uploads matlab files to numpy array using hdf5storage package\n",
    "# Unused channelas are removed and a dicitonary of all channels and scoring are returned\n",
    "def download_new_file(file_name):\n",
    "    file_dict=hdf5storage.loadmat(file_name)\n",
    "    keys_to_remove = ['Keyboard','Racine','file']\n",
    "    new_dict = {k: v for k, v in file_dict.items() if k not in keys_to_remove}\n",
    "    return new_dict\n",
    "\n",
    "# This function takes name of the recorded channel and compressees the data 10 times using \n",
    "# forward-backward filtering wit parameters  a, b stored in the config file\n",
    "# And normalizes the data using z-score (if ScoreAtDecimation is labeled as True in the config file)\n",
    "def decimate_channel(k, dict):\n",
    "    channel_data=np.array(dict[k]['values'][0],dtype=np.float32)\n",
    "    channel_data=signal.filtfilt(b,a,channel_data,axis=0)\n",
    "    if config.zScoreAtDecimation==True:\n",
    "        channel_data=(channel_data-np.mean(channel_data))/np.std(channel_data)\n",
    "\n",
    "    if 'ECog' in k:\n",
    "        order=2\n",
    "    else:\n",
    "        order=8\n",
    "    \n",
    "    dec_record=np.array(signal.decimate(channel_data, 10, n=order, ftype='iir',axis=0,zero_phase=True),dtype=np.float32)\n",
    "\n",
    "    return dec_record\n",
    "\n",
    "# This function goes through all the values in the dicitonary, applies decimate function\n",
    "# And returns a numpy array for all channels in orfder specified by \"channels\" list in conifg file\n",
    "\n",
    "def decimate_all_channels(new_array):\n",
    "    dec_dict={}\n",
    "    dh_keys.update(f'Keys: {new_array.keys()}')\n",
    "\n",
    "    for k in new_array.keys():\n",
    "        for c in channels:\n",
    "            if  c in k: \n",
    "                new_key=\"dec\"+c\n",
    "                dec_dict[new_key]=decimate_channel(k, new_array)\n",
    "                \n",
    "    min_channel_samples=len(min(dec_dict.values(), key=len))\n",
    "\n",
    "    flattened_arrays = [np.array(value).flatten()[:min_channel_samples] for value in dec_dict.values()]\n",
    "    \n",
    "    \n",
    "    l=np.array(flattened_arrays)\n",
    "    return l\n",
    "\n",
    "#Ths function reshapes the aray creating 2160 epochs with 4000 recordings (20s) each\n",
    "def create_epochs(dec_data):\n",
    "    \n",
    "    axis=-1\n",
    "    l=np.array(dec_data)\n",
    "    complete_epochs=config.epoch_samples_dec*np.floor(l.shape[1]/(config.epoch_samples_dec)).astype(int)\n",
    "    strict_epochs=l[:, :complete_epochs]\n",
    "    strict_epochs=zscore(strict_epochs,axis=-1)\n",
    "\n",
    "    \n",
    "    n_epochs=int(complete_epochs/config.epoch_samples_dec)\n",
    "    \n",
    "    c_arr = strict_epochs[:, :n_epochs*config.epoch_samples_dec].reshape(4, n_epochs, config.epoch_samples_dec).transpose(1, 0, 2)\n",
    "    dh_progress.update(f'{c_arr.shape}')\n",
    "    return c_arr\n",
    "\n",
    "# This function looks for EMG channel (specified as second column in the array)\n",
    "# And creates RMS function in 1 s intervals\n",
    "def calculate_EMG_RMS(decimated_array_data):\n",
    "    \n",
    "    \n",
    "# This is the function that creates RMS values for EMG dataframe\n",
    "    EMG_epochs=decimated_array_data[:,1, :]\n",
    "    bins=config.epoch_length                               # number of 1 second bins to use for RMS\n",
    "    EMG_window=int(config.epoch_samples_dec/config.epoch_length )  # 4000 is total number of samples/epoch - results in 1 sec rms bins at 200 Hz \n",
    "    # iterate over all epochs\n",
    "    EMG_rms=[]\n",
    "    \n",
    "    for epoch in range(EMG_epochs.shape[0]):\n",
    "        for bin in range(bins):\n",
    "            win1=int(bin*(EMG_window))                                 # beginning of window = bin * length of bin\n",
    "            win2=int(win1 + (EMG_window-1))  # beginning of window + length of bin - 1\n",
    "            emg_subset=EMG_epochs[epoch][win1:win2]     # extract given EMG from window\n",
    "            EMG_rms=np.append(EMG_rms, np.sqrt((emg_subset).mean()**2))    # perform root-mean-square on EMG from window\n",
    "    \n",
    "\n",
    "    EMG_rms=EMG_rms.reshape(EMG_epochs.shape[0],config.epoch_length)\n",
    "\n",
    "    \n",
    "    return EMG_rms\n",
    "\n",
    "\n",
    "#This function generates full set of features described in the paper \n",
    "#for all 4 channels\n",
    "# NOTE: in the original version of the file 108 colums instead of 100 are created\n",
    "\n",
    "def feature_generation(decimated_array_data):\n",
    "    sig=decimated_array_data\n",
    "    # Calculate all the datapoints per epoch per channel\n",
    "    sig_len=sig.shape[-1]\n",
    "\n",
    "    #Perform Fourier transformation\n",
    "    # get broadband FFT magnitude in bin 2-55 Hz for normalization\n",
    "    fourier_space = pyfftw.builders.fft(sig, axis=-1)\n",
    "    mag=abs(fourier_space()[:,:,0:sig_len])/sig_len*2\n",
    "    broadband=np.mean(mag[:,:,2:55], axis=-1)\n",
    "\n",
    "    #Perform PSD transformation\n",
    "    # get broadband PSD magnitude in bin 2-55 Hz for normalization\n",
    "    f, psd=signal.welch(sig, sampling_freq_dec, axis=-1)\n",
    "    broadband_psd=np.mean(psd[:, :, 2:55], axis=-1)\n",
    "\n",
    "\n",
    "    #Generate Features array based on transformations data and concatenate it with \n",
    "    # The array with EMG RMS data\n",
    "\n",
    "    # features_array= np.array([])\n",
    "    features_array =np.array([np.mean(sig, axis=-1), \n",
    "                        np.median(sig, axis=-1), \n",
    "                        np.std(sig, axis=-1), \n",
    "                        np.var(sig, axis=-1), \n",
    "                        skew(sig, axis=-1), \n",
    "                        kurtosis(sig, axis=-1), \n",
    "                        np.mean(mag[:, :, 2:4], axis=-1)/broadband,\n",
    "                        np.mean(mag[:, :, 4:7], axis=-1)/broadband,\n",
    "                        np.mean(mag[:, :, 7:13], axis=-1)/broadband,\n",
    "                        np.mean(mag[:, :, 13:30], axis=-1)/broadband,\n",
    "                        np.mean(mag[:, :, 30:55], axis=-1)/broadband,\n",
    "                        np.mean(mag[:, :,65:100], axis=-1)/broadband,\n",
    "                            np.mean(mag[:, :, 2:4], axis=-1)/np.mean(mag[:, :,4:7], axis=-1),\n",
    "                            # broadband,\n",
    "                            np.mean(psd[:, :, 2:4], axis=-1)/broadband_psd,\n",
    "                            np.mean(psd[:, :, 4:7], axis=-1)/broadband_psd,\n",
    "                            np.mean(psd[:, :, 7:13], axis=-1)/broadband_psd,\n",
    "                            np.mean(psd[:, :, 13:30], axis=-1)/broadband_psd,\n",
    "                            np.mean(psd[:, :, 30:55], axis=-1)/broadband_psd,\n",
    "                            np.mean(psd[:, :, 65:100], axis=-1)/broadband_psd,\n",
    "                            np.mean(psd[:, :, 2:4], axis=-1)/np.mean(psd[:, :,4:7],axis=-1),\n",
    "                            # broadband_psd\n",
    "                            ])\n",
    "    # CHange the shape of the array, adding the columns of all four channels as rows\n",
    "    f=features_array.transpose(1,2, 0).reshape(sig.shape[0],-1)\n",
    "    # Get RMS data\n",
    "    e=calculate_EMG_RMS(sig)\n",
    "    b = np.pad(e, pad_width=((0,0),(0,20-e.shape[-1])), mode='constant', constant_values=0)\n",
    "    #Concatenate both features arrays\n",
    "    x = np.concatenate((f, b), axis=1)\n",
    "\n",
    "    return x\n",
    "\n",
    "# This function looks for scoring data in the dictionary created from uploaded matlab data and reshape it to \n",
    "# concatenate it with the rest of decimated data\n",
    "\n",
    "def find_scores(new_dict):# This is code that looks for scoring array\n",
    "    a_reshaped=None\n",
    "    for k in new_dict.keys():\n",
    "        pattern = r'(?i)sl'\n",
    "        match = re.search(pattern, k)\n",
    "        if match:\n",
    "            # print(\"Found:\", match.group())\n",
    "            # print(np.array(new_dict[k]['codes'][0]).shape)\n",
    "\n",
    "            Sleep_codes=np.array(new_dict[k]['codes'][0])\n",
    "            epoch_ratio=round(target_epoch_count/len(Sleep_codes))\n",
    "            sleepEpochsRound=int(target_epoch_count/epoch_ratio)     \n",
    "                \n",
    "            a_reshaped = np.repeat(Sleep_codes[:sleepEpochsRound,0],epoch_ratio).reshape(target_epoch_count, 1)\n",
    "                \n",
    "            \n",
    "    return a_reshaped\n",
    "    \n",
    "\n",
    "# This function saves fully processed data into the provided path\n",
    "def save_processed_data(decimated_folder_path,file_name, x_data,y_data):\n",
    "\n",
    "\n",
    "    if type(y_data)==np.ndarray:\n",
    "        folder_path = decimated_folder_path+'npy_newest_scored/'+'Feats_Fourier_and_PSD/'\n",
    "    else:\n",
    "        folder_path = decimated_folder_path+'npy_newest_unscored/'+'Feats_Fourier_and_PSD/'\n",
    "\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    file_name=file_name.replace(\".smrx\", \"\").replace(\".mat\", \"\")\n",
    "    word_list = file_name.split(\" \")\n",
    "    name_x=f'x_ffnorm {word_list[0]} {word_list[1]}  {word_list[2]} {word_list[3]} {word_list[4]} {word_list[5]}'\n",
    "    \n",
    "    name_x=f'x_ffnorm {word_list[4]} {word_list[1]} {word_list[2]} {word_list[0]}'\n",
    "\n",
    "    name_y=f'y_ffnorm {word_list[4]} {word_list[1]} {word_list[2]} {word_list[0]}'\n",
    "    # Full path for the file\n",
    "    full_path_x = os.path.join(folder_path, name_x)\n",
    "    full_path_y = os.path.join(folder_path, name_y)\n",
    "\n",
    "\n",
    "    # Save the data to the specified file within the new folder\n",
    "    np.save(full_path_x, x_data)\n",
    "    if type(y_data)==np.ndarray:\n",
    "        np.save(full_path_y, y_data)\n",
    "    dh_name.update(print(f\"{file_name} processed and saved\"))\n",
    "\n",
    "def process_and_save(path):# Here the names of files to process are uploaded\n",
    "# THe list is formatted as [complete_path, annotation type folder name, file_name]\n",
    "\n",
    "\n",
    "    file_list=return_file_list_from_server(path)\n",
    "    file_names = file_list\n",
    "    for file in progressbar(file_names):\n",
    "        # print(file[0])    \n",
    "        \n",
    "        scored_folder_path = decimated_folder_path+'npy_newest_scored/'+'Feats_Fourier_and_PSD/'\n",
    "        unscored_folder_path = decimated_folder_path+'npy_newest_unscored/'+'Feats_Fourier_and_PSD/'\n",
    "\n",
    "        file_name=file[2].replace(\".smrx\", \"\").replace(\".mat\", \"\")\n",
    "        dh_name.update(f'processing {file_name}')\n",
    "        word_list = file_name.split(\" \")\n",
    "        name_x=f'x_ffnorm {word_list[4]} {word_list[1]} {word_list[2]} {word_list[0]}'\n",
    "        full_path_scored = os.path.join(unscored_folder_path, name_x+'.npy')\n",
    "        full_path_unscored = os.path.join(scored_folder_path, name_x+'.npy')\n",
    "        if os.path.isfile(full_path_unscored) or os.path.isfile(full_path_scored):\n",
    "            continue\n",
    "\n",
    "        new_array=download_new_file(file[0])\n",
    "    \n",
    "\n",
    "        \"\"\"Here the uploaded file is ordered and decimated and converted to numpy array\"\"\"\n",
    "        dec_data=decimate_all_channels(new_array)\n",
    "\n",
    "        error_flag = unit_tests_code.run_all_tests(dec_data)\n",
    "        if error_flag:\n",
    "            continue\n",
    "                \n",
    "\n",
    "        # For annotated data y is found and data is transformed into epochs\n",
    "        \n",
    "        array_epochs=create_epochs(dec_data)\n",
    "        result= feature_generation(array_epochs)\n",
    "        scores=find_scores(new_array)\n",
    "        #scores_data= np.repeat(scores, 4000, axis=2)\n",
    "        save_processed_data(decimated_folder_path,file[2], result, scores)\n",
    "        gc.collect()\n",
    "        time.sleep(.1)\n",
    "   \n",
    "\n",
    "    \n",
    "    print (\"All Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'processing NPM564 566-568 191024 070729_039 m3 NPM566 SlNo'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Keys: dict_keys(['V3_ECog', 'V3_EMG', 'V3_HPC_L', 'V3_HPC_R', 'V3_NPM567slslsl'])\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[████████████████████████████████████████████████████████████] 2/2 Est wait 00:0.0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Done!\n"
     ]
    }
   ],
   "source": [
    "global dh_name, dh_keys,dh_error, dh_progress\n",
    "\n",
    "dh_name = display(f'Item: ',display_id=True)\n",
    "dh_keys = display(f'Keys: ',display_id=True)\n",
    "dh_error = display(f'',display_id=True)\n",
    "dh_progress = display(f'',display_id=True)\n",
    "\n",
    "process_and_save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
