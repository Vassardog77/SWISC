{"cells":[{"cell_type":"markdown","metadata":{"id":"sm52p-D24JMJ"},"source":["# Initialize"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"hHHkZLtiDKyI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680893063112,"user_tz":420,"elapsed":6073,"user":{"displayName":"Brandon Harvey","userId":"12508064718982313309"}},"outputId":"6d8b406e-14a3-459a-db71-bce302a11bef"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.12.0\n","3.7.1\n"]}],"source":["# import packages \n","\n","%load_ext autoreload\n","\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","\n","import gc\n","import sys\n","import numpy as np\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","from tqdm import tqdm_notebook\n","import pickle\n","import logging\n","\n","# Imports for Deep Learning\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (mean_squared_error, confusion_matrix, \n","    ConfusionMatrixDisplay, classification_report, \n","    cohen_kappa_score, matthews_corrcoef)\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","import keras\n","from keras import metrics\n","from keras.regularizers import l1\n","from keras.models import Sequential, load_model\n","from keras import initializers\n","from keras.layers.convolutional import Conv1D, MaxPooling1D\n","from keras.layers import (Dense, Dropout, Activation, Flatten, Input, \n","                          TimeDistributed, Reshape, Permute, Flatten, \n","                          RepeatVector, Bidirectional, InputLayer,  \n","                          AlphaDropout, Normalization, MaxPooling2D, Embedding, \n","                          ConvLSTM1D, Attention, TimeDistributed, LocallyConnected1D,\n","                          LSTM, GRU)\n","from keras.models import Model, load_model\n","from keras import optimizers\n","from keras.utils.vis_utils import plot_model\n","from tensorflow.keras.optimizers import Nadam, Adam, SGD, Adadelta, Adamax, RMSprop\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n","import tensorflow as tf\n","\n","from collections import deque\n","import random\n","import math\n","import time \n","import datetime\n","\n","from keras.utils import np_utils\n","import shutil\n","\n","from sklearn.preprocessing import normalize, StandardScaler\n","from sklearn.decomposition import PCA\n","\n","import matplotlib\n","\n","# Imports for CSV Processing\n","from matplotlib.collections import LineCollection\n","from matplotlib.colors import ListedColormap, BoundaryNorm\n","import matplotlib.patches as mpatches\n","\n","print(keras.__version__)\n","print(matplotlib.__version__)\n","\n","%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZlMOWRF3X41","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679960460707,"user_tz":420,"elapsed":820,"user":{"displayName":"Brandon Harvey","userId":"12508064718982313309"}},"outputId":"cfa8b082-d10d-41e8-9549-980db7ad5400"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SgsbVTAIE7oT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679960460945,"user_tz":420,"elapsed":244,"user":{"displayName":"Brandon Harvey","userId":"12508064718982313309"}},"outputId":"8fde1a55-8998-47dc-c82f-a2c587d4cb2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n","GPU name:  []\n"]}],"source":["# Ensure GPU RAM is >10 GB\n","\n","!nvidia-smi\n","\n","print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fTYy8yQP7r5u"},"outputs":[],"source":["# Consistent random seed selection improves reliability of Keras training performance\n","tf.random.set_seed(42)\n","np.random.seed(42)"]},{"cell_type":"markdown","source":["Run the below cell if using git repo cloned to Google Drive"],"metadata":{"id":"4FLaWzmO9hq6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6eS-WBUGI1LE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679960460945,"user_tz":420,"elapsed":6,"user":{"displayName":"Brandon Harvey","userId":"12508064718982313309"}},"outputId":"5fc8b774-d29d-411f-e242-4964c4c58c3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n","/content/drive/MyDrive/Classifier Final\n"]}],"source":["%cd ''\n","basepath='./drive/MyDrive/Classifier Final/'\n","%cd '/content/drive/MyDrive/Classifier Final/'\n","path_Fourier_scored='./Variables/Feats_Fourier_and_PSD/'\n","path_Fourier_unscored='./Variables/Feats_Fourier_and_PSD_unscored/'\n","path_output='./CSV_Outputs/'\n","path_variables='./Variables/'"]},{"cell_type":"markdown","source":["Local variant of above cell"],"metadata":{"id":"9t-K2Nco9shJ"}},{"cell_type":"code","source":["# basepath='C:/Users/BHARVE4/Documents/deep-sleep-seizure/Classifier_Final'\n","# %cd $basepath\n","# path_Fourier_scored='./Variables/Feats_Fourier_and_PSD/'\n","# path_Fourier_unscored='./Variables/Feats_Fourier_and_PSD_unscored/'\n","# path_output='./CSV_Outputs/'\n","# path_variables='./Variables/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TKPdS1bQeFyC","executionInfo":{"status":"ok","timestamp":1679961021110,"user_tz":420,"elapsed":9,"user":{"displayName":"Brandon Harvey","userId":"12508064718982313309"}},"outputId":"40fb450e-f117-4a3e-9b1f-841e7b7be43c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["C:\\Users\\BHARVE4\\Documents\\deep-sleep-seizure\\Classifier_Final\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1yPsGaUgF9M1","colab":{"base_uri":"https://localhost:8080/","height":398},"executionInfo":{"status":"error","timestamp":1679960452443,"user_tz":420,"elapsed":57329,"user":{"displayName":"Brandon Harvey","userId":"12508064718982313309"}},"outputId":"09343c0c-9e7b-4e34-a5df-8eed6c0f797c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/Classifier Final/Variables/Feats_Fourier_and_PSD.zip\n","Archive:  /content/drive/MyDrive/Classifier Final/Variables/Feats_Fourier_and_PSD_unscored.zip\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-e1e786cf1479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Unzip Fourier Transformed variables to folders if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unzip '/content/drive/MyDrive/Classifier Final/Variables/Feats_Fourier_and_PSD.zip' -d '/content/drive/MyDrive/Classifier Final/Variables/'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unzip '/content/drive/MyDrive/Classifier Final/Variables/Feats_Fourier_and_PSD_unscored.zip' -d '/content/drive/MyDrive/Classifier Final/Variables/'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# !unzip '/content/drive/MyDrive/Classifier Final/Pretrained/reports.zip' -d '/content/drive/MyDrive/Classifier Final/Pretrained/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    454\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;31m# TODO(b/115527726): Rather than sleep, poll for incoming messages from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;31m# the frontend in the same poll as for the output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Unzip Fourier Transformed variables to folders if needed \n","# !unzip '/content/drive/MyDrive/Classifier Final/Variables/Feats_Fourier_and_PSD.zip' -d '/content/drive/MyDrive/Classifier Final/Variables/'\n","# !unzip '/content/drive/MyDrive/Classifier Final/Variables/Feats_Fourier_and_PSD_unscored.zip' -d '/content/drive/MyDrive/Classifier Final/Variables/'\n","# !unzip '/content/drive/MyDrive/Classifier Final/Pretrained/reports.zip' -d '/content/drive/MyDrive/Classifier Final/Pretrained/'"]},{"cell_type":"markdown","metadata":{"id":"IkdeE_ZT2-y9"},"source":["# Define Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcNfdlAB3A-c"},"outputs":[],"source":["def consecutive(data, stepsize=1):\n","    inds =  np.where(np.diff(data) != stepsize)[0]+1\n","    consec = np.split(data, inds)\n","    return consec, inds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KGFWQf-z3L5F"},"outputs":[],"source":["def unique(list1): \n","  \n","    # intilize a null list \n","    global unique_list  \n","    unique_list = [] \n","    \n","    # traverse for all elements \n","    for x in list1: \n","        # check if exists in unique_list or not \n","        if x not in unique_list: \n","            unique_list.append(x) \n","    # print list \n","    return sorted(unique_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ydr2zEvA3P5E"},"outputs":[],"source":["def load_Fourier_xy(path, file_list):\n","\n","    total_files = len(file_list)  # length of list\n","    print(total_files)\n","    \n","    epochs = 2160    \n","    total_epoch_count = int(epochs*(total_files)) # total length of data array in 10 second epochs\n","    \n","    # establish x and y vectors for input data\n","    dh1 = display(f'Items left: {total_files}',display_id=True)\n","    dh2 = display('Loading...',display_id=True)\n","\n","    # counting and progress flag variables \n","    y_done = 0\n","    x_done = 0\n","    file_counter = 0\n","    \n","    x = np.zeros((total_epoch_count, 100))\n","    y = np.zeros((total_epoch_count, 1))\n","    excl = 0\n","    # iterate over all Fourier transformed files\n","    for item in (sorted(file_list)):\n","        # print(item)\n","        total_files -= 1\n","        dh1.update(f'Items left: {total_files}')\n","        \n","        item = item.replace(\"x_ffnorm\", \"y_ffnorm\") \n","        current_file_y = np.load(f\"{path}{item}\")\n","        item = item.replace(\"y_ffnorm\", \"x_ffnorm\") \n","\n","        for i in current_file_y:\n","          if i not in [1,2,3,4,5]:\n","            excl=1\n","\n","        if excl==0:\n","            # load Fourier feature vectors\n","            current_file_x = np.load(f\"{path}{item}\")\n","            x[file_counter*epochs:((file_counter+1)*epochs)] = StandardScaler().fit_transform(current_file_x)\n","            x_done = 1\n","            \n","            # load Sleep/Seizure score vectors\n","            item = item.replace(\"x_ffnorm\", \"y_ffnorm\") \n","            current_file_y = np.load(f\"{path}{item}\")\n","            y[file_counter*epochs:((file_counter+1)*epochs)] = current_file_y-1\n","            y_done = 1\n","\n","            if x_done == 1 and y_done == 1:\n","                file_counter += 1\n","                dh2.update(f'loaded x and y for {item}')\n","                if file_counter%50 == 0:\n","                    print(file_counter)\n","                x_done = 0\n","                y_done = 0\n","\n","        else: \n","            print(f'{item} excluded')\n","            excl = 0\n","\n","    return x,y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j76F6-xP30q1"},"outputs":[],"source":["def norm_sklearn_classweight(y_train, mu=False):\n","    \n","    # Get maximum range of class labels \n","    classes = unique(np.argmax(y_train,1))\n","\n","    # Convert scores back from one-hot to ints\n","    y_train_classes = [np.argmax(z) for z in y_train]\n","    unique(y_train_classes)\n","\n","    \n","    # assign balanced class weights\n","    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_classes)\n","    print(weights)\n","    class_weight = {}\n","    ind = 0\n","\n","    \n","    # adjust weights based upon mu parameter or the minimum weight present\n","    for i in classes:\n","        j = int(np.where(classes==i)[0])\n","        print(j)\n","        if mu == False:\n","            score = weights[j]/min(weights)  \n","        else: \n","            score = math.log(weights[j]/mu)\n","        class_weight[j] = score if score > 1.0 else 1.0\n","\n","    return class_weight"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OK2i_YQn4RRP"},"outputs":[],"source":["def create_train_BiLSTM_model(X_train, X_test, y_train, y_test, class_weight, epochs, \n","                       steps_per_epoch=None, n_iter=200, batch_factor=.0001, prefix=\"\", layers=4, basepath=''):\n","        \n","        METRICS = ['accuracy',\n","          metrics.TruePositives(name='tp'),\n","          metrics.FalsePositives(name='fp'),\n","          metrics.TrueNegatives(name='tn'),\n","          metrics.FalseNegatives(name='fn'),\n","          metrics.CategoricalAccuracy(name='categorical_accuracy'),\n","          metrics.Precision(name='precision'),\n","          metrics.Recall(name='recall'),\n","          metrics.AUC(name='auc', curve=\"PR\"),\n","          metrics.CategoricalCrossentropy(name='categorical_crossentropy')]\n","      \n","        data_width = X_train.shape[1]\n","        data_depth = X_train.shape[2]\n","        shape = (data_width,data_depth)\n","\n","        batch_size = 2160\n","        LSTM_size = n_iter\n","        rs_flag=True\n","            \n","        # Model creation with optional stacking of Bi-LSTM/Dropout pairs\n","        model = Sequential()\n","        model.add(InputLayer(input_shape=shape))\n","\n","        for count in range(layers):\n","            if count==0:\n","              activity_regularizer=l1(0.0001)\n","            else:\n","              activity_regularizer=None\n","          \n","            factor = 2**(count)\n","            forward_layer = LSTM(int(LSTM_size/factor), activity_regularizer=activity_regularizer, return_sequences=rs_flag)\n","            backward_layer = LSTM(int(LSTM_size/factor), activity_regularizer=activity_regularizer, return_sequences=rs_flag,\n","                                  go_backwards=True)\n","            model.add(Bidirectional(forward_layer, backward_layer=backward_layer))\n","            model.add(Dropout(.4))\n","\n","        # Dense softmax prediction\n","        model.add(Flatten())     \n","        model.add(Dense(5))\n","        model.add(Activation('softmax'))\n","\n","\n","        # model parameters\n","        opt=Nadam(learning_rate=0.00001)\n","        \n","        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=METRICS)\n","        model.build()\n","        model.summary()\n","        \n","        # model logging parameters\n","        model_name=f\"BiLSTM_size_{LSTM_size}_{prefix}_\"\n","        filepath = f\"{model_name}\"+\"_weights.auc:{auc}-{categorical_accuracy:.6f}--val-{val_auc}-{val_categorical_accuracy:.6f}.h5\"\n","\n","        mcp = ModelCheckpoint(f\"./Checkpoints/{filepath}\", \n","                              monitor='val_loss', verbose=1, \n","                              save_best_only=True, save_weights_only=False, mode='min')       \n","                              \n","        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n","                    patience=5, min_delta=0.001,restore_best_weights=True)\n","                    \n","        log_dir = f\"./Logs/{model_name}\"\n","        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","        logpath=f\"./Logs/\"\n","        \n","        csv_path=f\"{model_name}_model.csv\"\n","        csv_logger=CSVLogger(logpath+csv_path)\n","        \n","        # call model.fit\n","        history = model.fit(X_train, y_train, batch_size=batch_size, class_weight=class_weight, epochs=epochs, \n","                            steps_per_epoch=steps_per_epoch,\n","                            validation_data=(X_test, y_test), verbose=2, callbacks=[es, mcp, csv_logger, tensorboard_callback])\n","\n","        return model, history, logpath, csv_path, model_name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbEevLAk0OtB"},"outputs":[],"source":["def create_train_LSTM_model(X_train, X_test, y_train, y_test, class_weight, epochs, \n","                       steps_per_epoch=None, n_iter=200, batch_factor=.0001, prefix=\"\", layers=4, basepath=''):\n","        \n","        METRICS = ['accuracy',\n","          metrics.TruePositives(name='tp'),\n","          metrics.FalsePositives(name='fp'),\n","          metrics.TrueNegatives(name='tn'),\n","          metrics.FalseNegatives(name='fn'),\n","          metrics.CategoricalAccuracy(name='categorical_accuracy'),\n","          metrics.Precision(name='precision'),\n","          metrics.Recall(name='recall'),\n","          metrics.AUC(name='auc', curve=\"PR\"),\n","          metrics.CategoricalCrossentropy(name='categorical_crossentropy')]\n","      \n","        data_width = X_train.shape[1]\n","        data_depth = X_train.shape[2]\n","        shape = (data_width,data_depth)\n","\n","        batch_size = 2160\n","        LSTM_size = n_iter\n","        rs_flag=True\n","            \n","        # Model creation with optional stacking of Bi-LSTM/Dropout pairs\n","        model = Sequential()\n","        model.add(InputLayer(input_shape=shape))\n","\n","        for count in range(layers):\n","            if count==0:\n","              activity_regularizer=l1(0.0001)\n","            else:\n","              activity_regularizer=None\n","            factor = 2**(count)\n","            forward_layer = LSTM(int(LSTM_size/factor), activity_regularizer=activity_regularizer, return_sequences=rs_flag)\n","            backward_layer = LSTM(int(LSTM_size/factor), activity_regularizer=activity_regularizer, return_sequences=rs_flag,\n","                                  go_backwards=True)\n","            model.add(forward_layer)\n","            model.add(Dropout(.4))\n","\n","        # Dense softmax prediction\n","        model.add(Flatten())     \n","        model.add(Dense(5))\n","        model.add(Activation('softmax'))\n","\n","\n","        # model parameters\n","        opt=Nadam(learning_rate=0.00001)\n","        \n","        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=METRICS)\n","        model.build()\n","        model.summary()\n","        \n","        # model logging parameters\n","        model_name=f\"Single_LSTM_size_{LSTM_size}_{prefix}_\"\n","        filepath = f\"{model_name}\"+\"_weights.auc:{auc}-{categorical_accuracy:.6f}--val-{val_auc}-{val_categorical_accuracy:.6f}.h5\"\n","\n","        mcp = ModelCheckpoint(f\"./Checkpoints/{filepath}\", \n","                              monitor='val_loss', verbose=1, \n","                              save_best_only=True, save_weights_only=False, mode='min')       \n","                              \n","        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n","                    patience=5, min_delta=0.001,restore_best_weights=True)\n","                    \n","        log_dir = f\"./Logs/{model_name}\"\n","        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","        logpath=f\"./Logs/\"\n","        \n","        csv_path=f\"{model_name}_model.csv\"\n","        csv_logger=CSVLogger(logpath+csv_path)\n","        \n","        # call model.fit\n","        history = model.fit(X_train, y_train, batch_size=batch_size, class_weight=class_weight, epochs=epochs, \n","                            steps_per_epoch=steps_per_epoch,\n","                            validation_data=(X_test, y_test), verbose=2, callbacks=[es, mcp, csv_logger, tensorboard_callback])\n","\n","        return model, history, logpath, csv_path, model_name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKj7p0GV5ewF"},"outputs":[],"source":["def create_train_flat_BiLSTM_model(X_train, X_test, y_train, y_test, class_weight, epochs, \n","                       steps_per_epoch=None, n_iter=8, batch_factor=.0001,prefix=\"\", layers=3,basepath=''):\n","\n","        METRICS = ['accuracy',\n","          metrics.TruePositives(name='tp'),\n","          metrics.FalsePositives(name='fp'),\n","          metrics.TrueNegatives(name='tn'),\n","          metrics.FalseNegatives(name='fn'),\n","          metrics.CategoricalAccuracy(name='categorical_accuracy'),\n","          metrics.Precision(name='precision'),\n","          metrics.Recall(name='recall'),\n","          metrics.AUC(name='auc', curve=\"PR\"),\n","          metrics.CategoricalCrossentropy(name='categorical_crossentropy')]         \n","\n","        data_width=X_train.shape[1]\n","   \n","        if len(X_train.shape)>2:\n","            data_depth=X_train.shape[2]\n","            shape=(data_width,data_depth)\n","        else: \n","            data_width=X_train.shape[1]\n","            shape=(data_width,)\n","            \n","        batch_size=2160\n","        LSTM_size=n_iter\n","        rs_flag=False\n","        \n","        # Model creation with optional stacking of Bi-LSTM/Dropout pairs\n","        model = Sequential()\n","        model.add(InputLayer(input_shape=shape))\n","        model.add(Flatten())           \n","        model.add(RepeatVector(1)) \n","\n","        # Each Bi-LSTM in cascade gets smaller by a factor of 2\n","        if layers in range(1,5):\n","            forward_layer = LSTM(int(LSTM_size), return_sequences=rs_flag)\n","            backward_layer = LSTM(int(LSTM_size), return_sequences=rs_flag,\n","                                  go_backwards=True)\n","            model.add(Bidirectional(forward_layer, backward_layer=backward_layer))\n","            model.add(RepeatVector(1)) \n","            model.add(Dropout(.4))\n","\n","            factor = 2\n","            if layers in range (2,5):\n","              forward_layer = LSTM(int(LSTM_size/factor), return_sequences=rs_flag)\n","              backward_layer = LSTM(int(LSTM_size/factor), return_sequences=rs_flag,\n","                                    go_backwards=True)\n","              model.add(Bidirectional(forward_layer, backward_layer=backward_layer))\n","              model.add(Dropout(.4))\n","              model.add(RepeatVector(1)) \n","\n","              factor = 4\n","              if layers in range (3,5):\n","                forward_layer = LSTM(int(LSTM_size/factor), return_sequences=rs_flag)\n","                backward_layer = LSTM(int(LSTM_size/factor), return_sequences=rs_flag,\n","                                      go_backwards=True)\n","                model.add(Bidirectional(forward_layer, backward_layer=backward_layer))\n","                model.add(Dropout(.4))\n","                model.add(RepeatVector(1)) \n","\n","                factor = 8\n","                if layers in range (4,5):\n","                    forward_layer = LSTM(int(LSTM_size/factor), return_sequences=rs_flag)\n","                    backward_layer = LSTM(int(LSTM_size/factor), return_sequences=rs_flag,\n","                                          go_backwards=True)\n","                    model.add(Bidirectional(forward_layer, backward_layer=backward_layer))\n","                    model.add(Dropout(.4))\n","                    model.add(RepeatVector(1)) \n","                \n","        # Dense softmax prediction\n","        model.add(Flatten())     \n","        model.add(Dense(5))\n","        model.add(Activation('softmax'))\n","\n","\n","        # model parameters\n","        opt=Nadam(learning_rate=0.00001)\n","        \n","        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=METRICS)\n","        model.build()\n","        model.summary()\n","        \n","        model_name=f\"flat_LSTM_size_{LSTM_size}_{prefix}_\"\n","        filepath = f\"{model_name}\"+\"_weights.auc:{auc}-{categorical_accuracy:.6f}--val-{val_auc}-{val_categorical_accuracy:.6f}.h5\"\n","\n","        # model logging parameters\n","        mcp = ModelCheckpoint(f\"./Checkpoints/{filepath}\", \n","                              monitor='val_loss', verbose=1, \n","                              save_best_only=True, save_weights_only=False, mode='min')       \n","                              \n","        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n","                    patience=5, min_delta=0.001,restore_best_weights=True)\n","                    \n","        log_dir = f\"./Logs/{model_name}\"\n","        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","        logpath=f\"./Logs/\"\n","        \n","        csv_path=f\"{model_name}_flat_model.csv\"\n","        csv_logger=CSVLogger(logpath+csv_path)\n","        \n","        # call model.fit\n","        history = model.fit(X_train, y_train, batch_size=batch_size, class_weight=class_weight, epochs=epochs, \n","                            steps_per_epoch=steps_per_epoch,\n","                            validation_data=(X_test, y_test), verbose=2, callbacks=[es, mcp, csv_logger, tensorboard_callback])\n","\n","\n","        return model, history, logpath, csv_path, model_name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p22gmaYp5iYk"},"outputs":[],"source":["def create_train_CNN_model(X_train, X_test, y_train, y_test, class_weight, epochs, \n","                       steps_per_epoch=None, n_iter=8, batch_factor=.0001,prefix=\"\", layers=3, basepath=''):\n","        \n","        METRICS = ['accuracy',\n","          metrics.TruePositives(name='tp'),\n","          metrics.FalsePositives(name='fp'),\n","          metrics.TrueNegatives(name='tn'),\n","          metrics.FalseNegatives(name='fn'),\n","          metrics.CategoricalAccuracy(name='categorical_accuracy'),\n","          metrics.Precision(name='precision'),\n","          metrics.Recall(name='recall'),\n","          metrics.AUC(name='auc', curve=\"PR\"),\n","          metrics.CategoricalCrossentropy(name='categorical_crossentropy')]\n","\n","        data_width=X_train.shape[1]\n","   \n","        if len(X_train.shape)>2:\n","            data_depth=X_train.shape[2]\n","            shape=(data_width,data_depth)\n","        else: \n","            shape=(data_width,)\n","            \n","        # parameters\n","        batch_size=2160\n","        CNN_size=n_iter\n","        rs=True\n","            \n","        # create CNN using Keras.Sequential\n","        model = Sequential()\n","        model.add(InputLayer(input_shape=shape))\n","        model.add(Conv1D(int(CNN_size),2))\n","        model.add(Flatten())           \n","        model.add(Dense(5))\n","        model.add(Activation('softmax'))\n","\n","        opt=Nadam(learning_rate=0.00001)\n","        \n","        # compile model\n","        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=METRICS)\n","        model.build()\n","        model.summary()\n","        \n","        # model logging parameters\n","        model_name=f\"CNN_size_{CNN_size}_{prefix}_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","        filepath = f\"{model_name}\"+\"_weights.auc:{auc}-{categorical_accuracy:.6f}--val-{val_auc}-{val_categorical_accuracy:.6f}.h5\"\n","        \n","        mcp = ModelCheckpoint(f\"./Checkpoints/{filepath}\", \n","                              monitor='val_loss', verbose=1, \n","                              save_best_only=True, save_weights_only=False, mode='min')       \n","                              \n","        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n","                    patience=5, min_delta=0.001,restore_best_weights=True)\n","                    \n","        log_dir = f\"./Logs/{model_name}\"\n","        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","        logpath=f\"./Logs/\"\n","        \n","        csv_path=f\"{model_name}_model.csv\"\n","        csv_logger=CSVLogger(logpath+csv_path)\n","        \n","        # call model.fit\n","        history = model.fit(X_train, y_train, batch_size=batch_size, class_weight=class_weight, epochs=epochs, \n","                            steps_per_epoch=steps_per_epoch,\n","                            validation_data=(X_test, y_test), verbose=2, callbacks=[es, mcp, csv_logger, tensorboard_callback])\n","\n","        return model, history, logpath, csv_path, model_name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eU1iiHC95mBs"},"outputs":[],"source":["def create_train_Dense_model(X_train, X_test, y_train, y_test, class_weight, epochs, \n","                       steps_per_epoch=None, n_iter=8, batch_factor=.0001,prefix=\"\", layers=3, basepath=''):\n","        \n","\n","        name_part='Both'\n","        data_width=X_train.shape[1]\n","   \n","        if len(X_train.shape)>2:\n","            data_depth=X_train.shape[2]\n","            shape=(data_width,data_depth)\n","        else: \n","            data_width=X_train.shape[1]\n","            shape=(data_width,)\n","            # shape=(20,data_width)\n","        Dense2_char=[]\n","            \n","        \n","        METRICS = [\n","        'accuracy',\n","        metrics.TruePositives(name='tp'),\n","        metrics.FalsePositives(name='fp'),\n","        metrics.TrueNegatives(name='tn'),\n","        metrics.FalseNegatives(name='fn'),\n","        metrics.CategoricalAccuracy(name='categorical_accuracy'),\n","        metrics.Precision(name='precision'),\n","        metrics.Recall(name='recall'),\n","        metrics.AUC(name='auc', curve=\"PR\"),\n","        metrics.CategoricalCrossentropy(name='categorical_crossentropy')]\n","        \n","        # batch_size=int((X_train.shape[0])*batch_factor)\n","        batch_size=2160\n","        layer_size_conv=64         \n","        LSTM_size=n_iter\n","        rs=True\n","            \n","        \n","        model = Sequential()\n","\n","        model.add(InputLayer(input_shape=shape))\n","        model.add(Dense(n_iter))\n","\n","        model.add(Flatten())           \n","\n","        model.add(Dense(5))\n","        model.add(Activation('softmax'))\n","\n","\n","        opt=Nadam(learning_rate=0.00001)\n","        \n","        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=METRICS)\n","        \n","        model.build()\n","\n","        model.summary()\n","        \n","        model_name=f\"Dense_size_{LSTM_size}_{prefix}_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","        filepath = f\"{model_name}\"+\"_weights.auc:{auc}-{categorical_accuracy:.6f}--val-{val_auc}-{val_categorical_accuracy:.6f}.h5\"\n","\n","        mcp = ModelCheckpoint(f\"./Checkpoints/{filepath}\", \n","                              monitor='val_loss', verbose=1, \n","                              save_best_only=True, save_weights_only=False, mode='min')\n","\n","\n","                        \n","        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n","                    patience=5, min_delta=0.0001,restore_best_weights=True)\n","\n","        \n","        log_dir = f\"./Logs/{model_name}\"\n","        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","        logpath=f\"./Logs/\"\n","        \n","        csv_path=f\"{model_name}_model.csv\"\n","        csv_logger=CSVLogger(logpath+csv_path)\n","        history = model.fit(X_train, y_train, batch_size=batch_size, class_weight=class_weight, epochs=epochs, \n","                            steps_per_epoch=steps_per_epoch,\n","                            validation_data=(X_test, y_test), verbose=2, callbacks=[es, mcp, csv_logger, tensorboard_callback])\n","\n","        return model, history, logpath, csv_path, model_name\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwelEPME5qSV"},"outputs":[],"source":["def save_model_report_seq(model, model_history, ctrls=0, run=2, x=[], y=[], X_train_seq=[], X_held_seq=[], y_train_seq=[], y_held_seq=[], X_test_seq=[], y_test_seq=[], model_name='test',i=1,model_num=8,basepath='',logpath='', csvpath='',savepath='./Results/'):  \n","  \n","  print(model_name)\n","  model_num=i*8\n","\n","  if ctrls==1:\n","    num_classes=3\n","  else:\n","    num_classes=5\n","\n","  if ctrls==0:\n","    charpath=f'Kainic Acid'\n","  elif ctrls==1:\n","    charpath=f'Control'\n","  elif ctrls==2:\n","    charpath='AllData'\n","  \n","  if os.path.exists(savepath)==False:\n","    os.mkdir(savepath)\n","  if os.path.exists(f'{savepath}Models/')==False:\n","    os.mkdir(f'{savepath}Models/')\n","  if os.path.exists(f'{savepath}Softmax/')==False:\n","    os.mkdir(f'{savepath}Softmax/')\n","\n","  codes=np.array(['wake','nrem','rem','seizure','post-ictal'])\n","\n","  # save model file\n","  # model.save(f'{savepath}Model_{model_name}.h5')  \n","\n","  if os.path.exists(f'{savepath}Models/{model_name}.h5')==False:\n","   model.save(f'{savepath}Models/{model_name}.h5')  \n","\n","  if model_history!=None:\n","    # save model history\n","    with open(f'{savepath}{model_name}{model_num}historydict','wb') as file_pi:\n","        pickle.dump(model_history.history, file_pi)\n","      \n","    shutil.copyfile(logpath+csvpath, f'{savepath}{csvpath}')\n","    \n","  # reconstitute entire dataset for evaluation\n","  x_new=np.concatenate((X_held_seq,X_test_seq, X_train_seq))\n","  y_new_1hot=np.concatenate((y_held_seq,y_test_seq, y_train_seq))\n","  \n","  # with model report file open \n","  with open(f'{savepath}Report_{model_name}.txt','w') as f:\n","    # iterate over train/test/val and combined datasets \n","    for selection in range(0,4):\n","        gc.collect()\n","        names=['Saline-KA Validation Dataset', 'Saline Control Dataset', 'Training Dataset', 'Train-Test-Val']\n","        X_check, y_check = [[X_held_seq, y_held_seq], [X_test_seq, y_test_seq],[X_train_seq, y_train_seq], [x_new, y_new_1hot]][selection]\n","  \n","        y_pred=model.predict(X_check, batch_size=2160)\n","        # y_pred.tofile(f'{savepath}Softmax/results_{model_name}.csv', sep=',')\n","\n","        #plot confusion matrices\n","        y_pred_codes=np.argmax(y_pred,1)\n","        y_test_codes=np.argmax(y_check,1)\n","        print(f'\\n {names[selection]} \\n', file=f)\n","        print(classification_report(y_test_codes, y_pred_codes), file=f)\n","\n","        dict1 = classification_report(y_test_codes, y_pred_codes, output_dict=True)\n","        df = pd.DataFrame(data=dict1)\n","        df = (df.T)\n","        print (df)\n","\n","        dataset_name=names[selection]\n","\n","        df.to_excel(f'{savepath}{dataset_name}{model_name}.xlsx')\n","        \n","        cm=confusion_matrix(y_test_codes,y_pred_codes,normalize='true')\n","\n","        print(f'unique ground truth labels: {unique(y_test_codes)}')\n","        print(f'unique predicted labels: {unique(y_pred_codes)}')\n","\n","\n","        if len(unique(y_pred_codes))>len(unique(y_test_codes)):\n","          disp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=codes[unique(y_pred_codes)])\n","        else:\n","          disp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=codes[unique(y_test_codes)])\n","\n","        disp.plot()\n","\n","        print(f'{names[selection]}')\n","\n","        plt.title(f'Confusion Matrix for {names[selection]}')\n","        plt.xlabel('Predicted')\n","        plt.ylabel('True')\n","        plt.savefig(f'{savepath}{model_name}_{names[selection]}_CM_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.png', dpi=700)\n","\n","        plt.close()\n","        gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QBWKQ3Xh5sv8"},"outputs":[],"source":["def class_matrix(model, ctrls=0, run=2, x=[], y=[], model_name='test',model_num=8, LSTM_size=None, basepath=''):\n","  run=f'Test_{run}_'\n","  if ctrls==1:\n","    num_classes=3\n","  else:\n","    num_classes=5\n","\n","  codes=[None]*5\n","  codes[0]='Wake'\n","  codes[1]='NREM'\n","  codes[2]='REM'\n","  codes[3]='Seizure'\n","  codes[4]='Post-Ictal'\n","\n","  labels_dict={}\n","\n","  if ctrls==0:\n","    charpath=f'Kainic Acid'\n","  elif ctrls==1:\n","    charpath=f'Control'\n","  elif ctrls==2:\n","    charpath='AllData'\n","\n","  savepath=f'./Trained/Final/'\n","\n","  model_num=100*8\n","\n","  Reportnum=1\n","  model_name=f\"LSTM_size_{LSTM_size}_{model_name}_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","\n","  model.save(f'{savepath}_Model_{model_name}{model_num}.h5')  \n","\n","  #savepath=f'{savepath}{model.name}/{charpath}/'\n","\n","  if os.path.exists(savepath)==False:\n","   os.mkdir(savepath)\n","\n","  Reportnum=1\n","  # with open(f'{savepath}{model_name}{model_num}historydict','wb') as file_pi:\n","  #     pickle.dump(model_history.history, file_pi)\n","\n","  x_new=np.concatenate((X_held_seq,X_test_seq, X_train_seq))\n","  y_new_1hot=np.concatenate((y_held_seq,y_test_seq, y_train_seq))\n","\n","\n","  tests=4\n","  with open(f'{savepath}Report_{model_name}_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.txt','w') as f:\n","    for sel in range(0,4):\n","        names=['Saline-KA Validation Dataset', 'Saline Control Dataset', 'Training Dataset', 'Train-Test-Val']\n","        X_check, y_check = [[X_held_seq, y_held_seq], [X_test_seq, y_test_seq],[X_train_seq, y_train_seq], [x_new, y_new_1hot]][sel]\n","\n","        y_pred=model.predict(X_check, batch_size=int(2160/2))\n","  \n","        y_pred_codes=np.argmax(y_pred,1)\n","        y_test_codes=np.argmax(y_check,1)\n","        print(f'\\n {names[sel]} \\n')\n","        f.write(classification_report(y_test_codes, y_pred_codes))\n","\n","        cm=confusion_matrix(y_test_codes,y_pred_codes, normalize='true')\n","\n","        disp=ConfusionMatrixDisplay(confusion_matrix=cm)\n","\n","        disp.plot()\n","        # plt.show()\n","\n","        print(f'{names[sel]}')\n","        #cax = ax.matshow(cm)\n","        plt.title(f'Confusion Matrix for {names[sel]}')\n","\n","        maxlabel=max(y_pred_codes)\n","\n","        if sel==1:\n","          maxlabel=3\n","\n","        disp.ax_.set_xticklabels(codes[0:maxlabel+1])\n","        disp.ax_.set_yticklabels(codes[0:maxlabel+1])\n","        plt.xlabel('Predicted')\n","        plt.ylabel('True')\n","        plt.savefig(f'{savepath}{model_name}_{names[sel]}_CM_.png', dpi=700)\n","\n","        plt.show()\n","        plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1JPqzdiLZjH"},"outputs":[],"source":["def generate_sequences(input_array, windows, x_or_y, max_feats=None, make_seq=True):\n","\n","  if make_seq==True:\n","\n","    if x_or_y not in ['X', 'Y']:\n","      \"Please designate whether input is X or Y array using x_or_y parameter\"\n","      return \n","\n","    for window in [windows]: # for loop to test varying window lengths\n","\n","\n","      classes=5\n","\n","      shift=window*2 # All x variable rows will be sampled with sliding sequences \n","      # If analyzing epoch 3, epochs from from -window_length (0) to \n","      # +window_length (6) around each epoch of interest will be sampled\n","      # Therefore, the windowing cannot begin earlier than epoch # [window_length]\n","\n","      # Windowing \n","\n","      if x_or_y in ['X']:\n","        if max_feats == None:\n","          max_feats=input_array.shape[1]\n","              # maximum number of features to use - use .shape[1] \n","              # of X_train, or the number 100, for full-featured training\n","        output_array=np.zeros((len(input_array)-shift,window*2+1,max_feats))\n","        for i in range(window,len(input_array)-window):\n","          output_array[i-window]=input_array[i-window:i+window+1,0:max_feats]\n","\n","\n","      if x_or_y in ['Y']:\n","        output_array=np.zeros((len(input_array)-shift, classes))\n","        for i in range(window,len(input_array)-window):\n","          output_array[i-window]=input_array[i]\n","\n","\n","      gc.collect()\n","\n","      return output_array\n","\n","  elif make_seq==False:\n","    print('no sequence generated')\n","    return input_array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dp6lyBvbG4r5"},"outputs":[],"source":["# Rechtshaffen and Kales scoring layer\n","\n","def R_and_K_evaluation(X_array_seq, model):\n","  if len(X_array_seq.shape)<2:\n","    X_array_seq=generate_sequences(X_array_seq, 3, x_or_y='X', max_feats=X_array_seq.shape()[1])\n","\n","  y_pred=model.predict(X_array_seq, batch_size=2160)\n","  y_pred_scores = np.argmax(y_pred,1)\n","\n","  y_pred_scores_baseline = y_pred_scores\n","\n","  for idx in range(0,len(y_pred_scores)):\n","    if idx>0:\n","      if y_pred_scores[idx]==2 and y_pred_scores[idx-1]==0:\n","        y_pred_scores[idx]=0\n","\n","      if y_pred_scores[idx]==1:\n","        if y_pred_scores[idx-1]!=1 and y_pred_scores[idx+1]!=1:\n","          y_pred_scores[idx]=y_pred_scores[idx-1]\n","\n","      if y_pred_scores[idx]==0:\n","        if y_pred_scores[idx-1]!=0 and y_pred_scores[idx+1]!=0:\n","          y_pred_scores[idx]=y_pred_scores[idx-1]\n","      \n","  agreement = y_pred_scores == y_pred_scores_baseline\n","\n","  agree_pct=len(agreement[agreement==True])/len(agreement)\n","  print(len(agreement[agreement==False]))\n","  print(agree_pct)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NHFrLvC_pMcy"},"outputs":[],"source":["def print_conf_mat(y_test_codes, y_pred_codes, legend, output_name):\n","\n","      \n","        labelmax=max(max(y_test_codes),max(y_pred_codes))\n","        print(labelmax)\n","        print(codes[0:labelmax+1])\n","\n","        disp=ConfusionMatrixDisplay.from_predictions(y_test_codes,y_pred_codes, labels=[0,1,2,3,4], normalize='true', display_labels=codes)\n","\n","        plt.title(f'{legend}')\n","        plt.xlabel('Predicted')\n","        plt.ylabel('True')\n","\n","        plt.show()\n","        plt.savefig(f'{output_name}_ConfMat.png', dpi=700)\n"]},{"cell_type":"code","source":["def score_data(path, errors, load_dropped_data=0, held_dates=[], scored=0):\n","    model.summary()\n","\n","    path_Fourier_scored='./Variables/Feats_Fourier_and_PSD/'\n","    path_Fourier_unscored='./Variables/Feats_Fourier_and_PSD_unscored/' \n","\n","    print(scored)\n","    if scored==1:\n","      strpart='trainset_classifier'\n","      path_Fourier=path_Fourier_scored\n","    elif scored==2:\n","      strpart='trainset_expert'\n","      path_Fourier=path_Fourier_scored\n","    else:\n","      strpart='classifier_prev_un'\n","      path_Fourier=path_Fourier_unscored\n","\n","\n","    %matplotlib inline\n","    score=0\n","    good=0\n","    path_csv=f'./CSV_Outputs/csv_{strpart}_scored/'\n","    if os.path.isdir(path_csv) == False:\n","      os.mkdir(path_csv)\n","\n","\n","    print(path_Fourier)\n","    fourier_files = [f for f in listdir(path_Fourier) if isfile(join(path_Fourier, f))]\n","\n","    fourier_files = [f for f in fourier_files if \"x_ffnorm\" in f]   # find all x arrays in fourier_files\n","\n","    print(held_dates)\n","\n","    if load_dropped_data==1:\n","        print('loading dropped data')\n","        fourier_files_2=[]\n","        for d in held_dates:\n","            fourier_files=[f for f in fourier_files if d in f]\n","        fourier_files_2=fourier_files\n","    else:\n","      for d in held_dates:\n","        fourier_files = [f for f in fourier_files if d not in f]\n","      fourier_files_2=fourier_files\n","\n","    test_length=len(fourier_files_2)  # length of list\n","    print(test_length)\n","\n","    total_epochs = int(2160*(test_length)) # total length of data array in 20 second epochs\n","\n","\n","    # establish x and y vectors for input data\n","    dh1 = display(f'Items left: {test_length}',display_id=True)\n","    dh = display('Loading...',display_id=True)\n","\n","    epochs=2160\n","\n","    y_done=0\n","    x_done=0\n","\n","    glob_counter = 0\n","\n","    for item in (sorted(fourier_files_2)):\n","        append=''\n","        x = np.zeros((epochs, 100))\n","\n","        y = np.zeros((epochs, 1))\n","\n","        test_length-=1\n","        dh1.update(f'Items left: {test_length}')\n","\n","        excl = 0\n","\n","        item = item.replace(\"x_ffnorm\", \"y_ffnorm\")\n","\n","        if os.path.isfile(f'{path_csv}{item}_scored.csv')==True:\n","          excl=1\n","\n","        item = item.replace(\"y_ffnorm\", \"x_ffnorm\")\n","\n","        if excl==0:\n","            sub=\"x_ffnorm\"\n","            ind=item.rfind(sub)\n","            item_dec=item[ind+2:]\n","            print(item_dec)\n","            current_file_x = StandardScaler().fit_transform(np.load(f\"{path_Fourier}{item}\"))\n","\n","            x_done=1\n","\n","            item = item.replace(\"x_ffnorm\", \"y_ffnorm\")\n","            if scored > 0:\n","              current_file_y = np.load(f\"{path_Fourier}{item}\")\n","              print(unique(current_file_y))\n","\n","            y_done=1\n","\n","            window=3\n","            shift=window*2\n","            max_feats=100\n","            classes=5\n","\n","            X_score3=np.zeros((len(current_file_x)-shift,window*2+1,max_feats))\n","            y_score3=np.zeros((len(current_file_x)-shift, classes))\n","\n","            for i in range(window,len(current_file_x)-window):\n","              X_score3[i-window]=current_file_x[i-window:i+window+1,0:max_feats]\n","              # y_score3[i-window]=y[i]\n","          \n","            if x_done == 1 and y_done == 1:\n","                glob_counter+=1\n","\n","                num_classes=5\n","\n","                if scored in [0,1]:\n","                  y_pred=model.predict(X_score3, 2160, verbose=0)\n","\n","                  y_pred_codes=np.argmax(y_pred,1)\n","\n","                else:\n","                  y_pred_codes=current_file_y-1\n","\n","                y_pred_codes.tofile(f'{path_csv}{item}_{strpart}_scored.csv', sep = ',')\n","\n","                dh.update(f'loaded x and y for {item}')\n","\n","                if glob_counter%50==0:\n","                    print(glob_counter)\n","                    print((score)/glob_counter)\n","                x_done=0\n","                y_done=0\n","\n","\n","\n","        else:\n","            print(f'{item} excluded')\n","            excl=0\n","\n"],"metadata":{"id":"IYF2ZGhKkzXM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def list_prune_via_substrings(list_to_be_scanned, substring_list):\n","  result_list=[]\n","\n","  if type(list_to_be_scanned[0])!=str:\n","    list_to_be_scanned=str(list_to_be_scanned)\n","\n","  if type(substring_list[0])!=str:\n","    substring_list=[str(sub) for sub in substring_list]\n","\n","  for scan_member in list_to_be_scanned:\n","    present=1\n","    for member in substring_list:\n","      if member in scan_member:\n","        present=0\n","    \n","    if present==1:\n","      result_list.append(scan_member)\n","\n","  return result_list"],"metadata":{"id":"0fDQ9CtMV6HY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tnlkiajy4Q8x"},"source":["# Load Data (alternate for testing)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rB_OXGr3TEZc","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1679961036560,"user_tz":420,"elapsed":7190,"user":{"displayName":"Brandon Harvey","userId":"12508064718982313309"}},"outputId":"96c59807-e063-4a9e-9295-76298f9a1bca"},"outputs":[{"output_type":"stream","name":"stdout","text":["[' NPM592 ', ' NPM596 ', ' NPM604 ', ' NPM614 ', ' NPM645 ', ' NPM646 ', ' NPM647 ', 'NPM656-659', ' NPM661 ', ' NPM662 ', ' NPM663 ', ' NPM666 ', ' NPM667 ', ' NPM668 ', '200316', '566', '569', ' NPM569 ', 'NPM570 ', ' NPM572 ', ' NPM574 ', ' NPM579 ', 'NPM573-576', ' NPM592', 'NPM644', 'NPM656', 'NPM652']\n","770\n"]},{"output_type":"display_data","data":{"text/plain":["'Items left: 363'"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["'loaded x and y for y_ffnorm NPM615 200829 191751_094 NPM612-615.npy'"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["50\n","100\n","150\n","200\n","250\n","300\n","350\n","400\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn [25], line 59\u001b[0m\n\u001b[0;32m     54\u001b[0m train_files\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     55\u001b[0m train_files\u001b[38;5;241m=\u001b[39m[f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fourier_files \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m files_held] \u001b[38;5;66;03m# gather list of all training files\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m x1,y1\u001b[38;5;241m=\u001b[39m\u001b[43mload_Fourier_xy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m saline_files\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     63\u001b[0m saline_files\u001b[38;5;241m=\u001b[39m[f \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m saline \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fourier_files \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m f] \u001b[38;5;66;03m# gather list of all saline VGAT Cre files for test dataset\u001b[39;00m\n","Cell \u001b[1;32mIn [11], line 49\u001b[0m, in \u001b[0;36mload_Fourier_xy\u001b[1;34m(path, file_list)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_done \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_done \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     48\u001b[0m     file_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 49\u001b[0m     \u001b[43mdh2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloaded x and y for \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mitem\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_counter\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28mprint\u001b[39m(file_counter)\n","File \u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\display_functions.py:374\u001b[0m, in \u001b[0;36mDisplayHandle.update\u001b[1;34m(self, obj, **kwargs)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;124;03m\"\"\"Update existing displays with my id\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m        additional keyword arguments passed to update_display\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 374\u001b[0m     update_display(obj, display_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\display_functions.py:326\u001b[0m, in \u001b[0;36mupdate_display\u001b[1;34m(obj, display_id, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m\"\"\"Update an existing display by id\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;124;03m:func:`display`\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    325\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m display(obj, display_id\u001b[38;5;241m=\u001b[39mdisplay_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\display_functions.py:305\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metadata:\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;66;03m# kwarg-specified metadata gets precedence\u001b[39;00m\n\u001b[0;32m    304\u001b[0m             _merge(md_dict, metadata)\n\u001b[1;32m--> 305\u001b[0m         publish_display_data(data\u001b[38;5;241m=\u001b[39mformat_dict, metadata\u001b[38;5;241m=\u001b[39mmd_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m display_id:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisplayHandle(display_id)\n","File \u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\display_functions.py:93\u001b[0m, in \u001b[0;36mpublish_display_data\u001b[1;34m(data, metadata, source, transient, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transient:\n\u001b[0;32m     91\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransient\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m transient\n\u001b[1;32m---> 93\u001b[0m display_pub\u001b[38;5;241m.\u001b[39mpublish(\n\u001b[0;32m     94\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m     95\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     97\u001b[0m )\n","File \u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\zmqshell.py:102\u001b[0m, in \u001b[0;36mZMQDisplayPublisher.publish\u001b[1;34m(self, data, metadata, transient, update)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish\u001b[39m(\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     82\u001b[0m     data,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m     update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     86\u001b[0m ):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;124;03m\"\"\"Publish a display-data message\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m        If True, send an update_display_data message instead of display_data.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flush_streams\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m {}\n","File \u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\zmqshell.py:65\u001b[0m, in \u001b[0;36mZMQDisplayPublisher._flush_streams\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flush_streams\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;124;03m\"\"\"flush IO Streams prior to display\"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n","File \u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\iostream.py:483\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m\"\"\"trigger actual zmq send\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03msend will happen in the background thread\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mthread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    481\u001b[0m ):\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# request flush on the background thread\u001b[39;00m\n\u001b[1;32m--> 483\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpub_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flush\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;66;03m# wait for flush to actually get through, if we can.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     evt \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mEvent()\n","File \u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel\\iostream.py:210\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    212\u001b[0m     f()\n","File \u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\zmq\\sugar\\socket.py:620\u001b[0m, in \u001b[0;36mSocket.send\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    613\u001b[0m         data \u001b[38;5;241m=\u001b[39m zmq\u001b[38;5;241m.\u001b[39mFrame(\n\u001b[0;32m    614\u001b[0m             data,\n\u001b[0;32m    615\u001b[0m             track\u001b[38;5;241m=\u001b[39mtrack,\n\u001b[0;32m    616\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    617\u001b[0m             copy_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_threshold,\n\u001b[0;32m    618\u001b[0m         )\n\u001b[0;32m    619\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mzmq\\backend\\cython\\socket.pyx:746\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mzmq\\backend\\cython\\socket.pyx:793\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mzmq\\backend\\cython\\socket.pyx:250\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n","File \u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["old_cohort = ['566']\n","\n","held_cohorts = ['200316','566', '569', ' NPM569 ', 'NPM570 ', ' NPM572 ', ' NPM574 ', ' NPM579 ',\n","                'NPM573-576',' NPM592', 'NPM644', 'NPM656', 'NPM652']\n","wild_types=['NPM648','NPM656', 'NPM652']\n","\n","wild_types_saline=['NPM644']\n","\n","saline=[' NPM592 ', ' NPM596 ',' NPM604 ', ' NPM614 ']\n","\n","\n","\n","# controls = [' NPM592 ',' NPM596 ', ' NPM604 ',' NPM609 ', 'NPM644-647']\n","recent = [' NPM645 ',' NPM646 ',' NPM647 ',\n","          'NPM656-659',\n","          ' NPM661 ',' NPM662 ', ' NPM663 ',\n","          ' NPM666 ', ' NPM667 ', ' NPM668 ']\n","\n","extra_train=[' NPM609 ',' NPM644 ', ' NPM664 ']\n","\n","held_dates = ['200103','200104','200105','200106',\n","       '200107','200108','200109',\n","       '200301','200302','200303',\n","       '200614','200615','200616',\n","       '200719','200720','200721',\n","       '200809','200810','200811',\n","       '201101','200102','201103',\n","       '201213','201214','201215',\n","       '210131','200201','210202',\n","       '210321','200322','210323',\n","       '200620','200621','200622']\n","\n","\n","test_data=saline+recent+held_cohorts\n","load_test_data=0\n","path=path_Fourier_scored\n","\n","\n","\n","\n","fourier_files = [f for f in listdir(path) if isfile(join(path, f))]\n","\n","fourier_files = [f for f in fourier_files if \"x_ffnorm\" in f]   # find all x arrays in fourier_files\n","\n","\n","non_train=saline+recent+held_cohorts\n","\n","\n","print(non_train)\n","files_held=[]\n","files_held=[f for d in non_train for f in fourier_files if d in f] # gather list of all non-training files\n","\n","\n","train_files=[]\n","train_files=[f for f in fourier_files if f not in files_held] # gather list of all training files\n","\n","\n","\n","x1,y1=load_Fourier_xy(path, train_files)\n","\n","\n","saline_files=[]\n","saline_files=[f for d in saline for f in fourier_files if d in f] # gather list of all saline VGAT Cre files for test dataset\n","x2,y2=load_Fourier_xy(path, saline_files)\n","\n","val_files=[]\n","val_files=[f for d in recent for f in fourier_files if d in f] # gather list of all validation files marked by 'recent' variable\n","x3,y3=load_Fourier_xy(path, val_files)\n"]},{"cell_type":"markdown","metadata":{"id":"5cSSIEeFS89D"},"source":["# Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZtFvvb7Q3PZ","colab":{"base_uri":"https://localhost:8080/","height":592},"executionInfo":{"status":"ok","timestamp":1679961058039,"user_tz":420,"elapsed":19160,"user":{"displayName":"Brandon Harvey","userId":"12508064718982313309"}},"outputId":"24a79e14-b863-4eeb-abd6-8cc2db51a7c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["[' NPM592 ', ' NPM596 ', ' NPM604 ', ' NPM614 ', 'NPM644', ' NPM645 ', ' NPM646 ', ' NPM647 ', 'NPM656-659', ' NPM661 ', ' NPM662 ', ' NPM663 ', ' NPM666 ', ' NPM667 ', ' NPM668 ', '200316', '566', '569', ' NPM569 ', 'NPM570 ', ' NPM572 ', ' NPM574 ', ' NPM579 ', 'NPM573-576', ' NPM580 ', 'NPM648', 'NPM652', 'NPM656']\n","719\n"]},{"output_type":"display_data","data":{"text/plain":["'Items left: 0'"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["'loaded x and y for y_ffnorm NPM665 210622 070057_015 NPM665-668.npy'"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["50\n","100\n","150\n","200\n","250\n","300\n","350\n","400\n","450\n","500\n","550\n","600\n","650\n","700\n","145\n"]},{"output_type":"display_data","data":{"text/plain":["'Items left: 0'"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["'loaded x and y for y_ffnorm NPM614 200811 191015_058 NPM612-615.npy'"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["50\n","100\n","323\n"]},{"output_type":"display_data","data":{"text/plain":["'Items left: 0'"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["'loaded x and y for y_ffnorm NPM668 210713 190351_058 NPM665-668.npy'"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["50\n","100\n","150\n","200\n","250\n","300\n"]}],"source":["# Datasets selected for Train/Test Split via this block\n","\n","old_cohort = ['566']  # not trained on animals with radically different signal\n","held_cohorts = ['200316','566', '569', ' NPM569 ', \n","                'NPM570 ', ' NPM572 ', ' NPM574 ', ' NPM579 ',\n","                'NPM573-576', ' NPM580 '] # Wild type or control mice with lack of baseline or classifier-breaking scoring issues \n","\n","wild_types=['NPM648','NPM652', 'NPM656'] # WT KA mouse cohort identifiers\n","wild_types_saline=['NPM644'] # WT saline mouse identifiers - no quotes ensures entire 644 cohort is loaded\n","saline = [' NPM592 ', ' NPM596 ',' NPM604 ', ' NPM614 '] # Saline \n","\n","\n","recent = [' NPM645 ',' NPM646 ',' NPM647 ', 'NPM656-659',\n","          ' NPM661 ',' NPM662 ', ' NPM663 ', ' NPM666 ', \n","          ' NPM667 ', ' NPM668 '] # Validation set of most recent animals\n","          # Mixed batch of KA and Saline, VGAT Cre and WT\n","\n","extra_train = [' NPM609 ',' NPM644 ', ' NPM664 '] \n","# random saline animals (609,644) and recent-KA animals (664) to add to training dataset\n","\n","held_dates = ['200103','200104','200105','200106',\n","       '200107','200108','200109',\n","       '200301','200302','200303',\n","       '200614','200615','200616',\n","       '200719','200720','200721',\n","       '200809','200810','200811',\n","       '201101','200102','201103',\n","       '201213','201214','201215',\n","       '210131','200201','210202',\n","       '210321','200322','210323',\n","       '200620','200621','200622']\n","       # all dates from baseline recordings of KA mice were ignored for training, \n","       # due to issues sorting by condition vs. mouse vs. date\n","\n","\n","\n","path=path_Fourier_scored\n","fourier_files = [f for f in listdir(path) if isfile(join(path, f))]\n","fourier_files = [f for f in fourier_files if \"x_ffnorm\" in f]   # find all x arrays in fourier_files\n","\n","non_train=saline+wild_types_saline+recent+held_cohorts+wild_types\n","\n","print(non_train)\n","files_held=[]\n","files_held=[f for d in non_train for f in fourier_files if d in f] # gather list of all non-training files\n","\n","\n","train_files=[]\n","train_files=[f for f in fourier_files if f not in files_held] # gather list of all training files\n","\n","\n","\n","x1,y1=load_Fourier_xy(path, train_files)\n","\n","\n","saline_files=[]\n","saline_files=[f for d in saline for f in fourier_files if d in f] # gather list of all saline VGAT Cre files for test dataset\n","x2,y2=load_Fourier_xy(path, saline_files)\n","\n","val_files=[]\n","val_files=[f for d in recent+wild_types for f in fourier_files if d in f] # gather list of all validation files marked by 'recent' variable\n","x3,y3=load_Fourier_xy(path, val_files)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VhqEbEnEc795"},"outputs":[],"source":["wild_type_list=[f for d in wild_types+wild_types_saline for f in fourier_files if d in f]\n","\n","names=[]\n","for i in wild_type_list:\n","  names.append(i[5:11])\n","\n","print(len(unique(names)))\n","\n","saline_list=[f for d in saline+wild_types_saline+[' NPM609 '] for f in fourier_files if d in f]\n","\n","names=[]\n","for i in saline_list:\n","  names.append(i[5:11])\n","\n","print(len(unique(names)))\n","\n","\n","# non_KA_list=saline+wild_types_saline+held_cohorts+[' NPM609 ']\n","# non_KA=[]\n","# non_KA=[f for d in non_KA_list for f in fourier_files if d in f] # gather list of all non-training files\n","# train_files=[]\n","# KA_list=[f for f in fourier_files if f not in non_KA] # gather list of all training files\n","\n","path=path_Fourier_scored\n","fourier_files = [f for f in listdir(path) if isfile(join(path, f))]\n","fourier_files = [f for f in fourier_files if \"x_ffnorm\" in f]   # find all x arrays in fourier_files\n","\n","non_train=saline+wild_types_saline+recent+held_cohorts+wild_types\n","\n","print(non_train)\n","files_held=[]\n","files_held=[f for d in non_train for f in fourier_files if d in f] # gather list of all non-training files\n","\n","\n","train_files=[]\n","train_files=[f for f in fourier_files if f not in files_held] # gather list of all training files\n","\n","names=[]\n","for i in train_files:\n","  names.append(i[9:15])\n","\n","l=unique(names)\n","print(*l, sep = \"\\n\")\n","print(len(unique(names)))\n","\n","names=[]\n","saline_files=[f for d in saline for f in fourier_files if d in f] # gather list of all saline VGAT Cre files for test dataset\n","for i in saline_files:\n","    names.append(i[9:15])\n","\n","print(*unique(names), sep = \"\\n\")\n","print(len(unique(names)))\n","\n","\n","\n","names=[]\n","val_files=[f for d in recent+wild_types for f in fourier_files if d in f] \n","for i in val_files:\n","    names.append(i[9:15])\n","\n","print(*unique(names), sep = \"\\n\")\n","print(len(unique(names)))"]},{"cell_type":"markdown","metadata":{"id":"SfSm9vvWIANC"},"source":["# Model Creation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUeqUiqtqDzz"},"outputs":[],"source":["# create array of codes to link numeric labels in y variables to state labels\n","\n","codes=[None]*5\n","codes[0]='wake'\n","codes[1]='nrem' \n","codes[2]='rem'\n","codes[3]='seizure'\n","codes[4]='post-ictal'\n","\n","# global variables for future functions\n","i=16\n","full=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vU07Qvwt5aU"},"outputs":[],"source":["keras.backend.clear_session()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5aWdlhnkPZV"},"outputs":[],"source":["# run before re-running training\n","\n","\n","X_train_seq=None\n","X_test_seq=None\n","X_held_seq=None\n","model=None\n","tf.keras.backend.clear_session()\n","tf.compat.v1.reset_default_graph()\n","# tf.reset_default_graph()\n","\n","for i in range(0,50):\n","  gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJt8K1u5biYz"},"outputs":[],"source":["gc.collect()\n","\n","%tensorboard --logdir f\"./Logs/\"\n"]},{"cell_type":"markdown","metadata":{"id":"P549XhudPl3_"},"source":["# Pre-Process X and Y Variables\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmgK2turA5hy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679961104711,"user_tz":420,"elapsed":18,"user":{"displayName":"Brandon Harvey","userId":"12508064718982313309"}},"outputId":"e2b8ab71-f443-4b21-97ff-45eaf79aac4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["28\n","48\n"]}],"source":["# These arrays correspond to lists of indices in the feature vector\n","# for each of our channels\n","\n","mask_DTRMS=np.concatenate(([12,19,32,39,52,59,72,79],np.arange(80,100)))\n","print(len(mask_DTRMS))\n","\n","mask_FFT_only = []\n","for i in [0,1,2,3]:\n","  ind1=(i*20)+6\n","  ind2=(i*20)+13\n","  mask_FFT_only=np.concatenate((mask_FFT_only,np.arange(ind1,ind2)))\n","\n","mask_FFT_only=np.concatenate((mask_FFT_only,np.arange(80,100)))\n","mask_FFT_only=[int(i) for i in mask_FFT_only]\n","print(len(mask_FFT_only))\n","\n","\n","mask_ECoG=np.arange(0,20)\n","mask_EMG=np.arange(20,40)\n","mask_HPCL=np.arange(40,60)\n","mask_HPCR=np.arange(60,80)\n","mask_RMS=np.arange(80,100)\n","\n","mask_FullFeats=np.arange(0,100)\n","\n","# Designate feature mask to use for preparing variables\n","# This construction would send just \"ECoG\" channel to training\n","\n","# mask=np.concatenate((mask_ECoG)) \n","mask = np.concatenate((mask_HPCL, mask_HPCR)) # mask_FullFeats lets all 100 features through\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTjoF5-a1bv_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679961107124,"user_tz":420,"elapsed":15,"user":{"displayName":"Brandon Harvey","userId":"12508064718982313309"}},"outputId":"e31c1be8-cbb1-4f46-fc0f-e90719fbe25b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[6, 7, 8, 9, 10, 11, 12, 26, 27, 28, 29, 30, 31, 32, 46, 47, 48, 49, 50, 51, 52, 66, 67, 68, 69, 70, 71, 72, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"]}],"source":["print(mask_FFT_only)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-5UFT46j1TT"},"outputs":[],"source":["# Convert ys to one-hot labels, store x variables to \n","# counteract future edits-in-place\n","\n","X_train=x1\n","y_train=np_utils.to_categorical(y1, num_classes=5)\n","\n","X_test=x2\n","y_test=np_utils.to_categorical(y2, num_classes=5)\n","\n","X_held=x3\n","y_held=np_utils.to_categorical(y3, num_classes=5)\n","\n","X_train_masked=X_train[:,mask]\n","X_test_masked=X_test[:,mask]\n","X_held_masked=X_held[:,mask]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iuU0atpvw-qn"},"outputs":[],"source":["X_train_masked.shape"]},{"cell_type":"markdown","metadata":{"id":"mG2GkKs0RsYz"},"source":["# SVM Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2b5PdJl5Cl6x"},"outputs":[],"source":["from IPython.lib.display import isdir\n","from sklearn.svm import LinearSVC\n","import sklearn\n","mu=.15\n","class_weight=norm_sklearn_classweight(y_train, mu=mu)\n","\n","savepath=f'./Results/CM_SVM/'\n","\n","if os.path.exists(savepath)==0:\n","  os.mkdir(savepath)\n","\n","\n","results=np.zeros((3,5,3))\n","feature_set_count=0\n","for masks in [[mask_DTRMS,'SVM: D/T Ratio & RMS EMG', 'DTRMS'],\n","              [mask_FFT_only, 'SVM: FFT, D/T \\n Ratio & RMS EMG', 'FFT'],\n","               [mask_none, 'SVM: Full Features', 'FullFeats']]:\n","\n","  print(f'\\n{masks[1]}')\n","  X_train_masked=X_train[:,masks[0]]\n","  X_test_masked=X_test[:,masks[0]]\n","  X_held_masked=X_held[:,masks[0]]\n","\n","  svm_model = sklearn.linear_model.SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, \n","                                                      fit_intercept=True, max_iter=1000, tol=0.001, shuffle=False, verbose=0, epsilon=0.1, n_jobs=None, \n","                                                      random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, \n","                                                      n_iter_no_change=5, class_weight=class_weight, warm_start=False, average=False)\n","\n","  svm_model.fit(X_train_masked, np.argmax(y_train,1))\n","\n","  \n","  for x, y, name in [[X_train_masked, y_train, ' Training Dataset'], [X_test_masked, y_test, ' Saline Control Dataset'], [X_held_masked, y_held, ' Saline-KA Validation Dataset']]:\n","    y_pred=svm_model.predict(x)\n","    \n","    print_conf_mat(np.argmax(y,1), y_pred, masks[1]+' \\n '+name, output_name=savepath+masks[2]+name)\n","\n","    y_pred_codes=y_pred\n","    y_test_codes=np.argmax(y,1)\n","\n","    print(classification_report(y_test_codes, y_pred_codes))\n","\n","    dict1 = classification_report(y_test_codes, y_pred_codes, output_dict=True)\n","    df = pd.DataFrame(data=dict1)\n","    df = (df.T)\n","    print (df)\n","\n","    df.to_excel(f'{savepath+masks[2]+name}.xlsx')\n","\n","\n","  for state_count in range(0,5):\n","    \n","    discriminant=np.argmax(y_train,1)==state_count\n","    if True in discriminant:\n","      results[feature_set_count,state_count,0] = svm_model.score(X_train_masked[discriminant], np.argmax(y_train,1)[discriminant])\n","    \n","    discriminant=np.argmax(y_test,1)==state_count\n","    if True in discriminant:\n","      results[feature_set_count,state_count,1] = svm_model.score(X_test_masked[discriminant], np.argmax(y_test,1)[discriminant])\n","\n","    discriminant=np.argmax(y_held,1)==state_count\n","    if True in discriminant:\n","      results[feature_set_count,state_count,2] = svm_model.score(X_held_masked[discriminant], np.argmax(y_held,1)[discriminant])\n","\n","  \n","  print(\"\\n\")\n","  feature_set_count+=1\n"]},{"cell_type":"code","source":["  for x, y, name in [[X_train_masked, y_train, ' Training Dataset'], [X_test_masked, y_test, ' Saline Control Dataset'], [X_held_masked, y_held, ' Saline-KA Validation Dataset']]:\n","    y_pred=svm_model.predict(x)\n","    \n","    print_conf_mat(np.argmax(y,1), y_pred, masks[1]+' \\n '+name, output_name=savepath+masks[2])\n","\n","    y_pred_codes=y_pred\n","    y_test_codes=np.argmax(y,1)\n","\n","    print(classification_report(y_test_codes, y_pred_codes))\n","\n","    dict1 = classification_report(y_test_codes, y_pred_codes, output_dict=True)\n","    df = pd.DataFrame(data=dict1)\n","    df = (df.T)\n","    print (df)\n","\n","    df.to_excel(f'{savepath+masks[2]+name}.xlsx')"],"metadata":{"id":"QxBVYCQ60sob"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ID8IuRaU36yZ"},"outputs":[],"source":["import matplotlib.patches as mpatches\n","plt.rcParams['figure.figsize']=[10,10]\n","\n","red_patch = mpatches.Patch(color='r', label='D/T Ratio + RMS EMG')\n","green_patch = mpatches.Patch(color='g', label='FFT Magn. only')\n","blue_patch = mpatches.Patch(color='b', label='Full Feature Set w PSDs')\n","\n","for i in range(3): \n","  shape_patch_1=matplotlib.lines.Line2D([],[],color=(0,0,0),marker='o', label='Train')\n","  shape_patch_2=matplotlib.lines.Line2D([],[],color=(0,0,0),marker='x', label='Test')\n","  shape_patch_3=matplotlib.lines.Line2D([],[],color=(0,0,0),marker='<', label='Validation')\n","\n","\n","for i in range(3):\n","  marker=['o','x','<'][i]\n","  for c in range(0,5):\n","    plt.scatter(['W','N','R','Sz','P'][c],results[0,c,i].T, marker=marker, color=(1,0,0), s=300, alpha=.6)\n","    plt.scatter(['W','N','R','Sz','P'][c],results[1,c,i].T, marker=marker, color=(0,1,0), s=300, alpha=.6)\n","    plt.scatter(['W','N','R','Sz','P'][c],results[2,c,i].T, marker=marker, color=(0,0,1), s=300, alpha=.6)\n","\n","plt.ylim([0,1])\n","plt.ylabel('Accuracy')\n","plt.title('SVM Performance')\n","plt.legend(handles=[red_patch, green_patch, blue_patch, shape_patch_1, shape_patch_2, shape_patch_3], bbox_to_anchor=(1.02, 1),\n","                  loc='upper left', borderaxespad=0.)\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUdXjgSr2-Lz"},"outputs":[],"source":["len(np.argmax(y_train,1)==i)\n"]},{"cell_type":"markdown","source":["# Full Deep Learning Grid Search"],"metadata":{"id":"bMX_QRQQsUMW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qWlTA2Hunjaf"},"outputs":[],"source":["mask_ECoG=np.arange(0,20)\n","mask_EMG=np.arange(20,40)\n","mask_HPCL=np.arange(40,60)\n","mask_HPCR=np.arange(60,80)\n","mask_RMS=np.arange(80,100)\n","\n","Channel_Masks=[[mask_ECoG, 'ECoG_only'],\n","               [np.concatenate((mask_HPCL,mask_HPCR)), '2xHPC'],\n","               [np.concatenate((mask_EMG,mask_HPCL,mask_HPCR,mask_RMS)), 'no_ECoG'],\n","               [np.concatenate((mask_EMG,mask_RMS)), 'EMG_RMS_only' ],\n","               [np.concatenate((mask_ECoG,mask_EMG,mask_RMS)), 'noHPC']]\n","\n","Feature_Masks = [[mask_DTRMS,'DTR+RMS only'],\n","  [mask_FFT_only, 'FFT only'],\n","  [mask_FullFeats, 'FullFeats']]\n","\n","for window_length in [0,1,2,3]:\n","  # Test all four windowing variants \n","  # Grid Search Iteration Count in This Loop: 4 Combinations\n","  for masks in Feature_Masks:\n","    # Test all three Feature Vectors \n","    # Grid Search Iteration Count in This Loop: 12 Combinations\n","\n","    print(f'\\n{masks[1]}')\n","    X_train_masked=X_train[:,masks[0]]\n","    X_test_masked=X_test[:,masks[0]]\n","    X_held_masked=X_held[:,masks[0]]\n","\n","    submodel=masks[1]\n","\n","    window_length=window_length # number of time points on either side of scored epoch to use\n","    # for input variables.  i.e. window_length 3 leads to an input 7 epochs long\n","\n","    max_feats=X_train_masked.shape[1]\n","\n","    X_train_seq=generate_sequences(X_train_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n","    X_test_seq=generate_sequences(X_test_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n","    X_held_seq=generate_sequences(X_held_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n","\n","    y_train_seq=generate_sequences(y_train, windows=window_length, x_or_y='Y', max_feats=max_feats)\n","    y_test_seq=generate_sequences(y_test, windows=window_length, x_or_y='Y', max_feats=max_feats)\n","    y_held_seq=generate_sequences(y_held, windows=window_length, x_or_y='Y', max_feats=max_feats)\n","\n","    print(X_train_seq.shape)\n","\n","    # Establishing variables for training\n","    layers=1 # number of Bi-LSTM layers\n","    nn=200 # target neuron count\n","    epochs=20 # number of epochs to train \n","    mu=.15 # mu factor of modified sklean_class_weight function\n","\n","\n","    print(mu)\n","    class_weight=norm_sklearn_classweight(y_train_seq, mu=mu)\n","\n","      \n","    print(class_weight)\n","\n","    for nn in [50,100,200]:\n","      # Test Three Initial-Layer Neuron Counts\n","      # Grid Search Iteration Count in This Loop: 36 Combinations\n","\n","      # Test All Above Over 4 Types of Models: 144 Total Model Variants\n","\n","\n","      gc.collect()\n","\n","      model, model_history, logpath, csvpath, model_name = create_train_BiLSTM_model(X_train_seq, X_test_seq, y_train_seq, y_test_seq, class_weight=class_weight, \n","                              epochs=epochs, steps_per_epoch=None, n_iter=nn, batch_factor=.001, layers=1, basepath=basepath)\n","      gc.collect()\n","\n","      savepath='./Results/BiLSTM/'\n","\n","      save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_held_seq=X_held_seq, y_train_seq=y_train_seq, y_held_seq=y_held_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n","                            model_name=model_name+f'single-layer_{submodel}_win{window_length}_', \n","                            model_num=nn, basepath=basepath, logpath=logpath, csvpath=csvpath, savepath=savepath)\n","      \n","\n","      model, model_history, logpath, csvpath, model_name = create_train_BiLSTM_model(X_train_seq, X_test_seq, y_train_seq, y_test_seq, class_weight=class_weight, \n","                              epochs=epochs, steps_per_epoch=None, n_iter=nn, batch_factor=.001, layers=4, basepath=basepath)\n","      gc.collect()\n","      savepath='./Results/Quad_BiLSTM/'\n","\n","      save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_held_seq=X_held_seq, y_train_seq=y_train_seq, y_held_seq=y_held_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n","                            model_name=model_name+f'quad-layer_{submodel}_win{window_length}_', \n","                            model_num=nn, basepath=basepath, logpath=logpath, csvpath=csvpath, savepath=savepath)\n","      \n","      \n","      model, model_history, logpath, csvpath, model_name = create_train_LSTM_model(X_train_seq, X_test_seq, y_train_seq, y_test_seq, class_weight=class_weight, \n","                              epochs=epochs, steps_per_epoch=None, n_iter=nn, batch_factor=.001, layers=1, basepath=basepath)\n","      gc.collect()\n","      savepath='./Results/LSTM/'\n","\n","      save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_held_seq=X_held_seq, y_train_seq=y_train_seq, y_held_seq=y_held_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n","                            model_name=model_name+f'_{submodel}_win{window_length}_', \n","                            model_num=nn, basepath=basepath, logpath=logpath, csvpath=csvpath, savepath=savepath)\n","      \n","    \n","    \n","      # model, model_history, logpath, csvpath, model_name = create_train_CNN_model(X_train_seq, X_test_seq, y_train_seq, y_test_seq, class_weight=class_weight, \n","      #                         epochs=epochs, steps_per_epoch=None, n_iter=nn, batch_factor=.001, layers=1, basepath=basepath)\n","      # gc.collect()\n","      # savepath='./Results/CNN/'\n","\n","\n","      # save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_held_seq=X_held_seq, y_train_seq=y_train_seq, y_held_seq=y_held_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n","      #             model_name=model_name+f'_{submodel}_win{window_length}', \n","      #             model_num=nn, basepath=basepath, logpath=logpath, csvpath=csvpath, savepath=savepath)\n","      \n","\n","\n","      # model, model_history, logpath, csvpath, model_name = create_train_flat_LSTM_model(X_train_seq, X_test_seq, y_train_seq, y_test_seq, class_weight=class_weight, \n","      #                         epochs=epochs, steps_per_epoch=None, n_iter=nn, batch_factor=.001, layers=1, basepath=basepath)\n","      # gc.collect()\n","      # save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_held_seq=X_held_seq, y_train_seq=y_train_seq, y_held_seq=y_held_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n","      #             model_name=model_name+f'_{submodel}_win{window_length}', \n","      #             model_num=nn, basepath=basepath, logpath=logpath, csvpath=csvpath, savepath=savepath)\n","      \n","\n","\n","      model, model_history, logpath, csvpath, model_name = create_train_Dense_model(X_train_seq, X_test_seq, y_train_seq, y_test_seq, class_weight=class_weight, \n","                              epochs=epochs, steps_per_epoch=None, n_iter=nn, batch_factor=.001, layers=1, basepath=basepath)\n","      gc.collect()\n","      savepath='./Results/Dense/'\n","\n","      save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_held_seq=X_held_seq, y_train_seq=y_train_seq, y_held_seq=y_held_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n","                            model_name=model_name+f'_{submodel}_win{window_length}', \n","                            model_num=nn, basepath=basepath, logpath=logpath, csvpath=csvpath, savepath=savepath)\n","\n"]},{"cell_type":"markdown","source":["# Evaluate Top Models to 60 Epochs"],"metadata":{"id":"Dg8f_hnUtyl-"}},{"cell_type":"code","source":["mask_ECoG=np.arange(0,20)\n","mask_EMG=np.arange(20,40)\n","mask_HPCL=np.arange(40,60)\n","mask_HPCR=np.arange(60,80)\n","mask_RMS=np.arange(80,100)\n","\n","Channel_Masks=[[mask_ECoG, 'ECoG_only'],\n","               [np.concatenate((mask_HPCL,mask_HPCR)), '2xHPC'],\n","               [np.concatenate((mask_EMG,mask_HPCL,mask_HPCR,mask_RMS)), 'no_ECoG'],\n","               [np.concatenate((mask_EMG,mask_RMS)), 'EMG_RMS_only' ],\n","               [np.concatenate((mask_ECoG,mask_EMG,mask_RMS)), 'noHPC']]\n","\n","Feature_Masks = [[mask_DTRMS,'DTR+RMS only'],\n","  [mask_FFT_only, 'FFT only'],\n","  [mask_FullFeats, 'FullFeats']]\n","\n","for window_length in [3]:\n","\n","  for masks in [Feature_Masks[2]]:\n","\n","    print(f'\\n{masks[1]}')\n","    X_train_masked=X_train[:,masks[0]]\n","    X_test_masked=X_test[:,masks[0]]\n","    X_held_masked=X_held[:,masks[0]]\n","\n","    submodel=masks[1]\n","\n","    window_length=window_length # number of time points on either side of scored epoch to use\n","    # for input variables.  i.e. window_length 3 leads to an input 7 epochs long\n","\n","    max_feats=X_train_masked.shape[1]\n","\n","    X_train_seq=generate_sequences(X_train_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n","    X_test_seq=generate_sequences(X_test_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n","    X_held_seq=generate_sequences(X_held_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n","\n","    y_train_seq=generate_sequences(y_train, windows=window_length, x_or_y='Y', max_feats=max_feats)\n","    y_test_seq=generate_sequences(y_test, windows=window_length, x_or_y='Y', max_feats=max_feats)\n","    y_held_seq=generate_sequences(y_held, windows=window_length, x_or_y='Y', max_feats=max_feats)\n","\n","    print(X_train_seq.shape)\n","\n","    # Establishing variables for training\n","    layers=1 # number of Bi-LSTM layers\n","    nn=200 # target neuron count\n","    epochs=60 # number of epochs to train \n","    mu=.15 # mu factor of modified sklean_class_weight function\n","\n","\n","    print(mu)\n","    class_weight=norm_sklearn_classweight(y_train_seq, mu=mu)\n","\n","      \n","    print(class_weight)\n","\n","    for nn in [50,100,200]:\n","\n","      gc.collect()\n","\n","      model, model_history, logpath, csvpath, model_name = create_train_BiLSTM_model(X_train_seq, X_test_seq, y_train_seq, y_test_seq, class_weight=class_weight, \n","                              epochs=epochs, steps_per_epoch=None, n_iter=nn, batch_factor=.001, layers=1, basepath=basepath)\n","      gc.collect()\n","\n","      savepath='./Results/BiLSTM/'\n","\n","      save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_held_seq=X_held_seq, y_train_seq=y_train_seq, y_held_seq=y_held_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n","                            model_name=model_name+f'single-layer_{submodel}_win{window_length}_', \n","                            model_num=nn, basepath=basepath, logpath=logpath, csvpath=csvpath, savepath=savepath)\n","      \n","\n","      model, model_history, logpath, csvpath, model_name = create_train_BiLSTM_model(X_train_seq, X_test_seq, y_train_seq, y_test_seq, class_weight=class_weight, \n","                              epochs=epochs, steps_per_epoch=None, n_iter=nn, batch_factor=.001, layers=4, basepath=basepath)\n","      gc.collect()\n","      savepath='./Results/Quad_BiLSTM/'\n","\n","      save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_held_seq=X_held_seq, y_train_seq=y_train_seq, y_held_seq=y_held_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n","                            model_name=model_name+f'quad-layer_{submodel}_win{window_length}_', \n","                            model_num=nn, basepath=basepath, logpath=logpath, csvpath=csvpath, savepath=savepath)\n","      \n","      \n","      model, model_history, logpath, csvpath, model_name = create_train_LSTM_model(X_train_seq, X_test_seq, y_train_seq, y_test_seq, class_weight=class_weight, \n","                              epochs=epochs, steps_per_epoch=None, n_iter=nn, batch_factor=.001, layers=1, basepath=basepath)\n","      gc.collect()\n","      savepath='./Results/LSTM/'\n","\n","      save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_held_seq=X_held_seq, y_train_seq=y_train_seq, y_held_seq=y_held_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n","                            model_name=model_name+f'_{submodel}_win{window_length}_', \n","                            model_num=nn, basepath=basepath, logpath=logpath, csvpath=csvpath, savepath=savepath)\n","      \n","    \n","\n"],"metadata":{"id":"PVeK2QXDsaxV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train Interpretable Machine Learning Models via Channel-Dropping"],"metadata":{"id":"2wuxMGcouipX"}},{"cell_type":"code","source":["mask_ECoG=np.arange(0,20)\n","mask_EMG=np.arange(20,40)\n","mask_HPCL=np.arange(40,60)\n","mask_HPCR=np.arange(60,80)\n","mask_RMS=np.arange(80,100)\n","\n","Channel_Masks=[[mask_ECoG, 'ECoG_only'],\n","               [np.concatenate((mask_HPCL,mask_HPCR)), '2xHPC'],\n","               [np.concatenate((mask_EMG,mask_HPCL,mask_HPCR,mask_RMS)), 'no_ECoG'],\n","               [np.concatenate((mask_EMG,mask_RMS)), 'EMG_RMS_only' ],\n","               [np.concatenate((mask_ECoG,mask_EMG,mask_RMS)), 'noHPC']]\n","\n","Feature_Masks = [[mask_DTRMS,'DTR+RMS only'],\n","  [mask_FFT_only, 'FFT only'],\n","  [mask_FullFeats, 'FullFeats']]\n","\n","for window_length in [3]:\n","\n","  for masks in Channel_Masks:\n","\n","    print(f'\\n{masks[1]}')\n","    X_train_masked=X_train[:,masks[0]]\n","    X_test_masked=X_test[:,masks[0]]\n","    X_held_masked=X_held[:,masks[0]]\n","\n","    submodel=masks[1]\n","\n","    window_length=window_length # number of time points on either side of scored epoch to use\n","    # for input variables.  i.e. window_length 3 leads to an input 7 epochs long\n","\n","    max_feats=X_train_masked.shape[1]\n","\n","    X_train_seq=generate_sequences(X_train_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n","    X_test_seq=generate_sequences(X_test_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n","    X_held_seq=generate_sequences(X_held_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n","\n","    y_train_seq=generate_sequences(y_train, windows=window_length, x_or_y='Y', max_feats=max_feats)\n","    y_test_seq=generate_sequences(y_test, windows=window_length, x_or_y='Y', max_feats=max_feats)\n","    y_held_seq=generate_sequences(y_held, windows=window_length, x_or_y='Y', max_feats=max_feats)\n","\n","    print(X_train_seq.shape)\n","\n","    # Establishing variables for training\n","    layers=1 # number of Bi-LSTM layers\n","    nn=200 # target neuron count\n","    epochs=60 # number of epochs to train \n","    mu=.15 # mu factor of modified sklean_class_weight function\n","\n","\n","    print(mu)\n","    class_weight=norm_sklearn_classweight(y_train_seq, mu=mu)\n","\n","      \n","    print(class_weight)\n","\n","    for nn in [200]:\n","\n","      gc.collect()\n","\n","      model, model_history, logpath, csvpath, model_name = create_train_BiLSTM_model(X_train_seq, X_test_seq, y_train_seq, y_test_seq, class_weight=class_weight, \n","                              epochs=epochs, steps_per_epoch=None, n_iter=nn, batch_factor=.001, layers=1, basepath=basepath)\n","      gc.collect()\n","\n","      savepath='./Results/Interpretable/'\n","\n","      save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_held_seq=X_held_seq, y_train_seq=y_train_seq, y_held_seq=y_held_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n","                            model_name=model_name+f'single-layer_{submodel}_win{window_length}_', \n","                            model_num=nn, basepath=basepath, logpath=logpath, csvpath=csvpath, savepath=savepath)\n","      \n","\n"],"metadata":{"id":"-DT5eRMUMiVf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nNFof7tMgHgN"},"source":["# Generate Classification Matrices for Previous Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lbBcpwnoSoir"},"outputs":[],"source":["# This block can be uncommented to quickly plot existing model sizes\n","mask=mask_FullFeats # mask_none lets all 100 features through\n","submodel=\"FullFeats\"\n","\n","X_train=X_train2[:,mask]\n","X_test=X_test2[:,mask]\n","X_held=X_held2[:,mask]\n","\n","window_length=3 # number of time points on either side of scored epoch to use\n","# for input variables.  i.e. window_length 3 leads to an input 7 epochs long\n","\n","X_train_seq=generate_sequences(X_train, windows=window_length, x_or_y='X', max_feats=100)\n","X_test_seq=generate_sequences(X_test, windows=window_length, x_or_y='X', max_feats=100)\n","X_held_seq=generate_sequences(X_held, windows=window_length, x_or_y='X', max_feats=100)\n","\n","y_train_seq=generate_sequences(y_train, windows=window_length, x_or_y='Y', max_feats=100)\n","y_test_seq=generate_sequences(y_test, windows=window_length, x_or_y='Y', max_feats=100)\n","y_held_seq=generate_sequences(y_held, windows=window_length, x_or_y='Y', max_feats=100)\n","\n","# model_folder='/content/drive/MyDrive/Classifier Final/Pretrained/Models/'\n","\n","# model_path=f'{model_folder}_Model_LSTM_size_25_full_20230110-215403800.h5'\n","# model=load_model(model_path)\n","# class_matrix(model, ctrls=2, run=2, x=None, y=None, model_name=f' {submodel}_ctrl_train_stdscaler_fftrmsemg_win{window}', model_num=0, LSTM_size=25, basepath=basepath)\n","\n","model_path='/content/drive/MyDrive/Classifier Final/Results/Final Model/Final Model BiLSTM200.h5'\n","model=load_model(model_path)\n","model.summary()\n","submodel='BiLSTM_200'\n","\n","save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, \n","                      X_train_seq=X_train_seq, X_held_seq=X_held_seq, y_train_seq=y_train_seq, \n","                      y_held_seq=y_held_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n","                      model_name=model_name+f'_{submodel}_win{window_length}', \n","                      model_num=nn, basepath=basepath, logpath=logpath, csvpath=csvpath)"]},{"cell_type":"markdown","source":["# R&K Scoring Layer"],"metadata":{"id":"1n443QNIbrrW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VdHbLo84u3Oj","colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"status":"ok","timestamp":1679961875796,"user_tz":420,"elapsed":62959,"user":{"displayName":"Brandon Harvey","userId":"12508064718982313309"}},"outputId":"fb735a13-832b-4aa6-8ec3-57d09ca209f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," bidirectional_4 (Bidirectio  (None, 7, 400)           481600    \n"," nal)                                                            \n","                                                                 \n"," dropout_4 (Dropout)         (None, 7, 400)            0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 2800)              0         \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 14005     \n","                                                                 \n"," activation_1 (Activation)   (None, 5)                 0         \n","                                                                 \n","=================================================================\n","Total params: 495,605\n","Trainable params: 495,605\n","Non-trainable params: 0\n","_________________________________________________________________\n"]},{"output_type":"display_data","data":{"text/plain":["'Item: 143'"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["1.0\n"]}],"source":["model=load_model(f'{basepath}/Results/Final Model/Final Model BiLSTM200.h5', compile=False)\n","model.compile()\n","\n","model.summary()\n","\n","agree_sum=[]\n","dh1 = display(f'Item: {0}',display_id=True)\n","\n","arr=X_test\n","\n","for i in range(int(len(arr)/2160)-1):\n","  dh1.update(f'Item: {i}')\n","  X_array_seq=[]\n","  X_array_seq=arr[i*2160:(i+1)*2160,:]\n","\n","  if len(X_array_seq.shape)<3:\n","    X_array_seq=generate_sequences(X_array_seq, 3, x_or_y='X', max_feats=X_array_seq.shape[1])\n","\n","  # print(X_array_seq.shape)\n","\n","  y_pred=model.predict(X_array_seq, batch_size=2160,verbose=0);\n","  y_pred_scores = np.argmax(y_pred,1)\n","\n","  y_pred_scores_baseline = y_pred_scores\n","\n","  # Rechtshaffen and Kales Critera\n","\n","  for idx in range(0,len(y_pred_scores)-1):\n","    if idx>0:\n","      # REM Must Follow NREM\n","      # \"If REM is preceded by Wake, score as Wake\"\n","      if y_pred_scores[idx]==2 and y_pred_scores[idx-1]==0:\n","        y_pred_scores[idx]=0\n","\n","      # In order to score NREM, there must be 2 consecutive epochs\n","      # or lone NREM is scored as the previous epoch\n","      if y_pred_scores[idx]==1:\n","        if y_pred_scores[idx-1]!=1 and y_pred_scores[idx+1]!=1:\n","          y_pred_scores[idx]=y_pred_scores[idx-1]\n","\n","      # In order to score Wake, there must be 2 consecutive epochs\n","      # or lone Wake is scored as the previous epoch\n","      if y_pred_scores[idx]==0:\n","        if y_pred_scores[idx-1]!=0 and y_pred_scores[idx+1]!=0:\n","          y_pred_scores[idx]=y_pred_scores[idx-1]\n","      \n","  agreement = y_pred_scores == y_pred_scores_baseline\n","\n","  agree_pct=len(agreement[agreement==True])/len(agreement)\n","  # print(len(agreement[agreement==False]))\n","  # print(agree_pct)\n","  agree_sum.append(agree_pct)\n","\n","\n","print(np.mean(agree_sum))\n"]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iES5I8Rqhf2Y","executionInfo":{"status":"ok","timestamp":1679961783924,"user_tz":420,"elapsed":25,"user":{"displayName":"Brandon Harvey","userId":"12508064718982313309"}},"outputId":"37fc53b3-b00f-43fd-98b3-d2c5055cbffb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([], dtype=int64),)"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","source":["# Final Scoring of Data"],"metadata":{"id":"1tRbb2-2buLN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWfVHX9J9bAA"},"outputs":[],"source":["model=load_model('/content/drive/MyDrive/Classifier Final/Results/Final Model/Final Model BiLSTM200.h5')\n","model.compile()\n","\n","model.summary()\n","\n","held_cohorts = []\n","\n","\n","# held_dates=held_cohorts+controls+held_dates\n","held_dates=[]\n","\n","scoretypes=[[0,'unscored'],[1,'class_scored'],[2,'expert_scored']]\n","\n","errors=[]\n","\n","for flag, types in scoretypes:\n","  print(f'Generating CSVs for {types}')\n","  score_data(path, errors, load_dropped_data=0, held_dates=[], scored=flag)\n"]},{"cell_type":"markdown","source":["# Process CSVs into Figures"],"metadata":{"id":"CWQ_7mFtC0ye"}},{"cell_type":"code","source":["csv_folder='/content/drive/MyDrive/Classifier Final/CSV_Outputs/'\n","path_csv=f'{csv_folder}csv_classifier_prev_un_scored/'\n","path_holes=f'{csv_folder}csv_trainset_classifier_scored/'\n","mouse_sort_csvs=f'{csv_folder}csv_full/'\n","path_expert=f'{csv_folder}csv_trainset_expert_scored/'\n","path_figs=f'{csv_folder}figures/'\n","\n","for folder in [path_csv, path_holes, mouse_sort_csvs, path_expert, path_figs]:\n","  if os.path.exists(folder)==0:\n","    os.mkdir(folder)"],"metadata":{"id":"ur7ksSSCDClB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for sel in [0,1,2]:\n","\n","  filepath = [path_csv,path_holes,path_expert][sel]\n","  filelist=os.listdir(filepath)\n","  print(len(filelist))\n","  print(sorted(filelist))\n","  count=0\n","  if count==0:\n","    dh1 = display(f'Items left: {len(filelist)}',display_id=True)\n","\n","  for item in sorted(filelist)[::-1]:\n","    if '.csv' in item:\n","\n","      count+=1    \n","      \n","      if count%50==0 or (len(filelist)-count)==0:\n","        dh1.update(f'Items left: {len(filelist)-count}')\n","\n","      ## Extract Mouse Name\n","      ind_name = item.find('NPM')\n","      name = item[ind_name:ind_name+6]\n","      \n","      path_out=f'{mouse_sort_csvs}{name}/'\n","\n","      if os.path.exists(path_out)==0:\n","        os.mkdir(path_out)\n","\n","      if os.path.isfile(f'{path_out}{item}')==0:\n","        shutil.copy(f'{filepath}{item}', f'{path_out}{item}')"],"metadata":{"id":"-BTCzd_VIDTx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plot Agreement Between Expert and Classifier"],"metadata":{"id":"BpkCJZLiWyFt"}},{"cell_type":"code","source":["# from re import A\n","plt.rcParams['figure.figsize'] = [10, 5]\n","import matplotlib.style as mplstyle\n","mplstyle.use('fast')\n","\n","\n","\n","folderlist=sorted(os.listdir(mouse_sort_csvs))\n","folderlist=[f for f in sorted(os.listdir(mouse_sort_csvs)) if '.csv' not in f]\n","print(folderlist)\n","\n","\n","use_expert=0\n","\n","if use_expert==1:\n","  skipped_type='classifier_scored'\n","  use_type='expert_scored'\n","\n","  score_label='Expert Scoring'\n","else:\n","  skipped_type='expert_scored'\n","  use_type='classifier_scored'\n","\n","  score_label='Trained or Tested Data'\n","\n","# print(folderlist)\n","\n","KA_list=[605,606,607,608,610,611,612,\n","         613,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,636,637,638,639,640,642,648,649,650,651,652,653,\n","         654,655,656,657,658,659,661,662,663,664,665,666,667,668,688,690,691,693,695,696,697]\n","\n","Excluded=[554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,573,576,577,578,579,589,591,593,594,595,580,605,606,607,\n","          608,609,610,611,612,613,614,615,616,617,618,630,631,635,637,641,642,643,648,651,655,657,658,662,663]\n","# Controls: 644,645,646,647,\n","slope_sum=np.zeros((4,1))\n","avg_slope=np.zeros((4,1))\n","\n","n_animals=0\n","\n","Controls=True\n","\n","\n","\n","for folder in folderlist[-22::-1]:\n","  \n","  Control=0\n","  KA_flag=False\n","  skip=0\n","  bad_flag=0\n","  Sz_occur=0\n","  mouse_folder = f'{mouse_sort_csvs}{folder}/'\n","  filelist=sorted(os.listdir(mouse_folder))\n","\n","  count_state = np.zeros((len(filelist),6))\n","  count_state_total = np.zeros((len(filelist),5))\n","  cumu_sum = np.zeros((len(filelist),5))\n","\n","  if os.path.isdir(f'{path_figs}{folder}')==False:\n","    os.mkdir(f'{path_figs}{folder}')\n","\n","  filelist = [f for f in filelist if skipped_type in f]   \n","  for KA in KA_list:\n","    if f'{KA}' in folder:\n","      KA_flag=True\n","\n","  for Excl in Excluded:\n","    if f'{Excl}' in folder:\n","      skip=1\n","\n","  if KA_flag!=Controls and skip==0:\n","    print(folder)\n","    sz=0\n","    total = len(filelist)\n","    # print(total)\n","    arr = np.zeros((total,2))\n","    count=0\n","\n","    if len(filelist)>0:\n","      for item in filelist[::-1]:\n","        if use_type in item:\n","           count_state[count][5]=1\n","        print(item)\n","        path = open(f'{mouse_folder}{item}')\n","\n","        if 'expert_scored' in item:\n","          array2 = np.loadtxt(path, delimiter=\",\",dtype='float')\n","          array2=array2.astype('int')\n","\n","        item2=item.replace(\"expert\", \"classifier\")\n","\n","        # print(item2)\n","        path = open(f'{mouse_folder}{item2}')\n","\n","        # print(item)\n","        if 'expert_scored' not in item2:\n","          array = np.loadtxt(path, delimiter=\",\",dtype='int')   \n","\n","        path.close()\n","\n","        name_ind=item.find('NPM')\n","        time_ind=name_ind+14\n","        if item[time_ind:time_ind+2]=='19':\n","          Z_time='night'\n","        else:\n","          Z_time='day'\n","\n","        # print(len(array2))\n","\n","        x1 = np.arange(0,2160)\n","        \n","        width=16\n","        height=width/4\n","        plt.figure(figsize=(width,height), dpi=2000)\n","        ax1=plt.subplot(2,4, (1,3))  \n","\n","        x_legend_offset=-.1\n","\n","        agreement = array==array2[3:2160-3]\n","\n","        agreement_score=np.zeros(len(agreement))\n","        agreement_score[agreement==True]=1\n","        agreement_score[agreement==False]=0\n","\n","        agree_pct=len(agreement[agreement==True])/(2160-6)\n","        # print(len(agreement[agreement==False]))\n","        print(agree_pct)\n","# Ax1\n","        \n","        ax1.bar(x1[3:2160-3], agreement_score,color='k', width=1)\n","        ax1.bar(np.ma.masked_where(~agreement, x1[3:2160-3]), np.ma.masked_where(agreement, agreement_score+1), color='grey', width=1)\n","\n","        # ax1.title.set_text(f'Sleep Record Scoring Comparision')\n","\n","        black_patch = mpatches.Patch(color='k', label='Agreement')\n","        grey_patch = mpatches.Patch(color='grey', label='Difference')\n","\n","        ax1.legend(handles=[black_patch, grey_patch], bbox_to_anchor=(1.02, 1),\n","                         loc='upper left', borderaxespad=0.)\n","        ax1.text(x_legend_offset, .5, f'Classifier and \\n Expert \\n Agreement', horizontalalignment='center', \n","                 verticalalignment='center', transform=ax1.transAxes)\n","        ax1.set_xticks(range(0,2160+int(2160/12),int(2160/12)))\n","\n","        if Z_time=='night':\n","          time_vect=[str(x)+\":00\" for x in list(range(19,24))+list(range(0,8))]\n","        else:\n","          time_vect=[str(x)+\":00\" for x in range(7,20)]\n","\n","\n","        ax1.set_xticklabels(time_vect)\n","\n","# Ax2\n","\n","        ax2=plt.subplot(2,4,(5,7))  \n","\n","        codes=['PI', 'Sz', 'W', 'N', 'R']\n","        types=max(array)\n","        # print(f'max legend = {types}')        \n","        \n","        array[array==4]=-2\n","        array[array==3]=-1\n","\n","        array2[array2==4]=-2\n","        array2[array2==3]=-1\n","\n","        ax2.plot(x1[3:2160-3], array, 'k')\n","        ax2.plot(x1[3:2160-3], array2[3:2160-3], color='grey')\n","\n","# agreement plotting\n","\n","        grey_patch = mpatches.Patch(color='grey', label='Expert Scoring')\n","        black_patch = mpatches.Patch(color='k', label='Classifier Scoring')\n","        ax2.legend(handles=[black_patch, grey_patch], bbox_to_anchor=(1.02, 1),\n","                         loc='upper left', borderaxespad=0.)\n","\n","        ax2.text(x_legend_offset, .5, f'Hypnogram', horizontalalignment='center', verticalalignment='center', transform=ax2.transAxes)\n","        ax2.set_xticks(range(0,2160+int(2160/12),int(2160/12)))\n","        ax2.set_xticklabels(time_vect)\n","        ax2.set_xlabel(\"Time\")\n","        if Z_time=='night':\n","          ax2.set_facecolor(\"lightgray\")\n","\n","# y tick assignment      \n","        \n","        ticklist=ax2.get_yticks()\n","        # print(ticklist)\n","        low_legend=min([min(array), min(array2)])\n","        # print(f'low legend = {low_legend}')        \n","\n","        high_legend=max([max(array), max(array2)])\n","        # print(f'high legend = {high_legend}')\n","\n","\n","\n","        ax2.set_yticks(range(-2,3))\n","        ax2.set_yticklabels(labels=codes)\n","        ax2.invert_yaxis()        \n","\n","\n","# Ax3\n","        ax3=plt.subplot(2,4, (4))\n","        ax3.pie([agree_pct,1-agree_pct], colors=['k', 'grey'],autopct='%1.1f%%', pctdistance=1.7, center=(0,-5))\n","        pie = ax3.get_position()\n","        pie.y0 = pie.y0 - 0.05\n","        pie.y1 = pie.y1 - 0.05\n","\n","        ax3.set_position(pie)\n","                \n","\n","\n","        # plt.plot(x1[3:2160-3], array, 'b--')\n","        # plt.plot(x1, array2, 'r-')\n","        # plt.plot(x1[3:2160-3], agreement_score)\n","        plt.show()\n","\n","        figname=f'{path_figs}{folder}/{item}_agreement.jpg'\n","        plt.savefig(figname, bbox_inches='tight')\n","\n","        count+=1\n","\n","      "],"metadata":{"id":"9YbVxBz9JXpT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Kaplan-Meier Seizure Graph\n"],"metadata":{"id":"zfh2lCKPR-az"}},{"cell_type":"code","source":["# from re import A\n","plt.rcParams['figure.figsize'] = [10, 5]\n","\n","folderlist=sorted(os.listdir(mouse_sort_csvs))\n","folderlist=[f for f in sorted(os.listdir(mouse_sort_csvs)) if '.csv' not in f]\n","print(folderlist)\n","\n","max_disp_files=38\n","\n","use_expert=1\n","\n","if use_expert==1:\n","  skipped_type='classifier_scored'\n","  use_type='expert_scored'\n","\n","  score_label='Expert Scoring'\n","else:\n","  skipped_type='expert_scored'\n","  use_type='classifier_scored'\n","\n","  score_label='Trained or Tested Data'\n","\n","# print(folderlist)\n","\n","KA_list=[605,606,607,608,610,611,612,\n","        613,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,642,648,649,650,651,652,653,\n","        654,655,656,657,658,659,661,662,663,664,665,666,667,668,688,690,691,693,695,696,697]\n","\n","KA_list_pre_cannula_move=[554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,573,575,576,577,578,579,580,590,589,590,591,593,594,595,605,606,607,\n","          608,610,611,612,613,615,616,617,618]\n","\n","Excl_KA=[568,573,577,578,591,594,616,618,619,623,627,630,631,637,641,648,649,650,651,652,653,654,655,656,657,658,659,660,662,665]\n","Excluded=[630,631,637,641,643,648,651,655,657,658,662,663]\n","\n","KA_list=KA_list+ KA_list_pre_cannula_move\n","Excluded=Excl_KA+Excluded\n","\n","for Controls in [False]:\n","\n","  folderlist=list_prune_via_substrings(folderlist, Excluded)\n","  if Controls==True:\n","    target_folders=list_prune_via_substrings(folderlist, KA_list)\n","\n","  if Controls==False:\n","    Control_folders=list_prune_via_substrings(folderlist, KA_list)\n","    target_folders=list_prune_via_substrings(folderlist, Control_folders)\n","\n","  slope_sum=np.zeros((4,1))\n","  avg_slope=np.zeros((4,1))\n","  \n","  num_mice=len(target_folders)    \n","  aggregate_sz=np.zeros((num_mice,max_disp_files))\n","\n","  n_animals=0\n","\n","  for folder in target_folders[:]:\n","    Control=False\n","    skip=0\n","    bad_flag=0\n","    KA_flag=False\n","    Sz_occur=0\n","\n"," \n","\n","    mouse_folder = f'{mouse_sort_csvs}{folder}/'\n","    filelist=sorted(os.listdir(mouse_folder))\n","\n","    filelist = [f for f in filelist if skipped_type not in f]   \n","    print(folder)\n","\n","    sz=0\n","    total = len(filelist)\n","    arr = np.zeros((total,2))\n","    count=0\n","\n","\n","    name_ind=filelist[0].find('NPM')\n","\n","    date_ind=name_ind+7\n","    date_record=int(filelist[0][date_ind:date_ind+6])\n","    if date_record < 191206:\n","      injection_file=0*2\n","    elif date_record < 200217:\n","      injection_file=34*2\n","    elif date_record < 200608:\n","      injection_file=16*2\n","    else:\n","      injection_file=9*2\n","\n","    filelist=filelist[injection_file:injection_file+max_disp_files]\n","\n","\n","    count_state = np.zeros((len(filelist),6))\n","    count_state_total = np.zeros((len(filelist),5))\n","    cumu_sum = np.zeros((len(filelist),5))\n","\n","    if len(filelist)>0:\n","      print(len(filelist))\n","      for item in filelist[:]:\n","        if use_type in item:\n","          count_state[count][5]=1\n","        path = open(f'{mouse_folder}{item}')\n","        if 'expert_scored' not in item:\n","          array = np.loadtxt(path, delimiter=\",\",dtype='int')\n","\n","        if 'expert_scored' in item:\n","          array = np.loadtxt(path, delimiter=\",\",dtype='float')\n","          array=array.astype('int')\n","\n","        ### Counting Loop\n","        first_inds=[]\n","        for i in range(5):\n","            ind_pres=np.where(array==i)[0]\n","            grp_s_ind, inds=consecutive(ind_pres)\n","\n","            first_s=[]\n","\n","            for i in range(len(grp_s_ind)):\n","                first_s.append(grp_s_ind[i])\n","\n","            first_s=np.asarray(first_s, order='K')\n","            first_inds.append((first_s))\n","        codes=unique(array)\n","        if 3 in codes:\n","          Sz_occur=1\n","        \n","        for state in range(5):\n","          statecount=np.zeros((len(first_inds[state]),1))\n","          for state_burst in range(len(first_inds[state])):\n","            count_state_total[count][state]=count_state_total[count][state]+(len(first_inds[state][state_burst]))\n","\n","          count_state[count][state]=len(first_inds[state])-1\n","\n","          if count==0:\n","            cumu_sum[count][state]=count_state[count][state]\n","            \n","\n","\n","\n","          else:\n","            cumu_sum[count][state]=cumu_sum[count-1][state]+count_state[count][state]          \n","            \n","          if state==3:\n","            if count <= max_disp_files:\n","              aggregate_sz[n_animals,count]+=cumu_sum[count][state]\n","\n","        count+=1\n","    n_animals+=1\n","        \n","      #######    \n","      \n","  fig, ax = plt.subplots()\n","  axes = [ax, ax.twinx()]\n","\n","  for i in range(n_animals):\n","    xmax=60\n","    ymax=300\n","    ymax2=60\n","    tick_marks=14\n","\n","    # n_animals+=1\n","    ax4 = axes[-1]\n","    x1 = np.arange(0,len(filelist))\n","\n","    # scored=count_state[:,5]==1     \n","    # unscored=count_state[:,5]==0\n","\n","    scored=count_state[:,5]==1     \n","    unscored=count_state[:,5]==0\n","\n","    scoredstyle='k'\n","    scoredstyle=[np.random.rand(),np.random.rand(),np.random.rand()]\n","    unscoredstyle='r'\n","    x_legend_offset=-.15\n","\n","    # if n_animals==1:\n","    #   \n","    # ax4 = plt.subplot(111)\n","    # sz_ratio=cumu_sum[:,3]\n","    # y=count_state[:,3]\n","    y=np.log(aggregate_sz[i,:])\n","    # ax4.plot(sz_ratio)\n","\n","    # print(len(count_state))\n","    # print(len(scored))\n","    # print(len(unscored))\n","    # print(len(x1))\n","    # print(len(y))\n","    \n","    if len(scored)>0:\n","      # ax4.plot(np.ma.masked_where(scored, x1), np.ma.masked_where(scored, y), 'r')\n","      ax4.plot(x1,y,c=scoredstyle)\n","    # if len(unscored)>0:\n","    #   ax4.plot(np.ma.masked_where(unscored, x1), np.ma.masked_where(unscored, y),unscoredstyle)\n","    # ax4.set(xlim=(0,xmax))\n","    # ax4.set(ylim=(0,20))\n","    ax4.grid(True,'minor','x',markevery=tick_marks)\n","    # ax4.set_yscale('log')\n","\n","    # ax5 = plt.subplot(515)\n","    # ax5.scatter(sum(count_state[:,3]), sum(count_state[:,0]))\n","    # ax5.set(xlim=(0,xmax))\n","    # # ax5.set(ylim=(0,ymax2))\n","    # ax5.grid(True,'major','x',markevery=tick_marks)\n","\n","    # ax5.set_yscale('log') \n","    red_patch = mpatches.Patch(color='red', label=score_label)\n","\n","    black_patch = mpatches.Patch(color='k', label='Classifier Scoring')\n","\n","    ax4.legend(handles=[red_patch, black_patch], loc='upper left')\n","\n","\n","    ax4.text(x_legend_offset, .5, f'Seizure onset \\n per 12h', horizontalalignment='center',\n","    verticalalignment='center', transform=ax4.transAxes)\n","    ax4.text(.5, -.3, f'Recording session (12 hr)', horizontalalignment='center', \n","    verticalalignment='center', transform=ax4.transAxes)\n","    ax4.title.set_text(f'{folder} \\n State Transitions Per Recording Session')\n","\n","    # if KA_flag==0:\n","    # plt.show()\n","    # plt.savefig(f'{path_figs}{use_type}frag_{folder}.jpg')\n","    # plt.show()\n","    # plt.close()\n","\n","\n","\n","\n","  # plt.show()\n","  ax4=axes[0]\n","\n","  xmax=60\n","  ymax=300\n","  ymax2=60\n","  tick_marks=14\n","\n","  n_animals+=1\n","\n","  x1 = np.arange(0,len(filelist))\n","\n","  # scored=count_state[:,5]==1     \n","  # unscored=count_state[:,5]==0\n","\n","  scored=count_state[:,5]==1     \n","  unscored=count_state[:,5]==0\n","\n","  scoredstyle='k'\n","  unscoredstyle='r'\n","  x_legend_offset=-.15\n","\n","  # if n_animals==1:\n","  #   \n","  # ax4 = plt.subplot(111)\n","  # sz_ratio=cumu_sum[:,3]\n","  # y=count_state[:,3]\n","  y=sum(aggregate_sz[0:,:])\n","  # ax4.plot(sz_ratio)\n","  \n","  if len(scored)>0:\n","    # ax4.plot(np.ma.masked_where(scored, x1), np.ma.masked_where(scored, y), 'r')\n","    ax4.plot(x1,y,scoredstyle)\n","  # if len(unscored)>0:\n","  #   ax4.plot(np.ma.masked_where(unscored, x1), np.ma.masked_where(unscored, y),unscoredstyle)\n","  # ax4.set(xlim=(0,xmax))\n","  # ax4.set(ylim=(0,20))\n","  ax4.grid(True,'minor','x',markevery=tick_marks)\n","  # ax4.set_yscale('log')\n","\n","  # ax5 = plt.subplot(515)\n","  # ax5.scatter(sum(count_state[:,3]), sum(count_state[:,0]))\n","  # ax5.set(xlim=(0,xmax))\n","  # # ax5.set(ylim=(0,ymax2))\n","  # ax5.grid(True,'major','x',markevery=tick_marks)\n","\n","  # ax5.set_yscale('log') \n","  red_patch = mpatches.Patch(color='red', label=score_label)\n","\n","  black_patch = mpatches.Patch(color='k', label='Classifier Scoring')\n","\n","  ax4.legend(handles=[red_patch, black_patch], loc='upper left')\n","\n","\n","  ax4.text(x_legend_offset, .5, f'Seizure onset \\n per 12h', horizontalalignment='center',\n","  verticalalignment='center', transform=ax4.transAxes)\n","  ax4.text(.5, -.3, f'Recording session (12 hr)', horizontalalignment='center', \n","  verticalalignment='center', transform=ax4.transAxes)\n","  ax4.title.set_text(f'Sum for all Animals - Sz Transitions Per Recording Session')\n","\n","  plt.show()\n","\n","  # ax1 = plt.subplot(511)  \n","  # wake_ratio=count_state[:,0]\n","  # ax1.plot(wake_ratio)\n","  #gridlines for day/week\n","  #72 hr after seizure\n","\n","  # if Controls==False:\n","  #   figname=f'{path_figs}holes_paper_cs_frag_KA.jpg'\n","  # else:\n","  #   figname=f'{path_figs}holes_paper_cs_frag_Controls.jpg'\n","\n","  # print(figname)\n","  # plt.savefig(figname, bbox_inches='tight')"],"metadata":{"id":"ABsR83wbR836"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Regenerate Class Matrices from Tested Models"],"metadata":{"id":"CrTxZ8oDv7Rw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FguvhD9Rntyp"},"outputs":[],"source":["# for type_folder in ['./Results/BiLSTM/Models/', './Results/LSTM/Models/','./Results/Dense/Models/','./Results/Quad_BiLSTM/Models/']:\n","for type_folder in ['./Results/Dense/Models/','./Results/Quad_BiLSTM/Models/']:\n","\n","  for file_name in sorted(os.listdir(type_folder))[:]:\n","    if '.h5' in file_name:\n","      print(file_name)\n","      model_name=file_name.replace('.h5', '')\n","      model=load_model(f'{type_folder}{file_name}')\n","\n","      if 'DTR+RMS only' in file_name:\n","        mask=mask_DTRMS\n","      elif 'FFT only' in file_name:\n","        mask=mask_FFT_only\n","      elif 'FullFeats' in file_name:\n","        mask=mask_FullFeats\n","\n","\n","      if 'win0' in file_name:\n","        window=0\n","      elif 'win1' in file_name:\n","        window=1\n","      elif 'win2' in file_name:\n","        window=2\n","      elif 'win3' in file_name:\n","        window=3\n","\n","      print(mask, window)\n","\n","      model.compile\n","\n","      X_train_masked=X_train[:,mask]\n","      X_test_masked=X_test[:,mask]\n","      X_held_masked=X_held[:,mask]\n","\n","\n","      window_length=window # number of time points on either side of scored epoch to use\n","      # for input variables.  i.e. window_length 3 leads to an input 7 epochs long\n","\n","      max_feats=X_train_masked.shape[1]\n","\n","      X_train_seq=generate_sequences(X_train_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n","      X_test_seq=generate_sequences(X_test_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n","      X_held_seq=generate_sequences(X_held_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n","\n","      y_train_seq=generate_sequences(y_train, windows=window_length, x_or_y='Y', max_feats=max_feats)\n","      y_test_seq=generate_sequences(y_test, windows=window_length, x_or_y='Y', max_feats=max_feats)\n","      y_held_seq=generate_sequences(y_held, windows=window_length, x_or_y='Y', max_feats=max_feats)\n","\n","      savepath=type_folder.replace('Models/','')\n","\n","      save_model_report_seq(model, model_history=None, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_held_seq=X_held_seq, y_train_seq=y_train_seq, y_held_seq=y_held_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n","                  model_name=model_name, \n","                  model_num=None, basepath=basepath, logpath=None, csvpath=None, savepath=savepath);\n","\n","      gc.collect()\n","\n","runtime.unassign()\n","\n","  \n","  "]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1SGyzMONFBtYkfsWAkNFYxl7Y-7mrmp7n","timestamp":1671046770467},{"file_id":"1omIYsg5uSiJuMlLRZQsB4jAtN-rybuxe","timestamp":1669239307423},{"file_id":"1yB1ssoUeicnSWFhQLxxXt8V_Ix5gwzzz","timestamp":1654806526194}],"collapsed_sections":["Tnlkiajy4Q8x"],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}